{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4a2f7f2",
   "metadata": {},
   "source": [
    "\n",
    "# `validate_npz_enhanced.md`\n",
    "\n",
    "## 📌 Overview\n",
    "\n",
    "`validate_npz_enhanced.py` is a **data quality auditing tool** for preprocessed `.npz` keypoint files.\n",
    "It validates whether each file is suitable for downstream training with **Transformer** and **IV3-GRU** models, generates structured reports, and (optionally) creates keypoint animations for visual QA.\n",
    "\n",
    "This script is designed for use in the `flsr-transformer-vs-iv3gru/` project, but the checks are generic enough for other pose/keypoint datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ What it does\n",
    "\n",
    "* Loads `.npz` files containing keypoints, masks, timestamps, and optional `X2048` features.\n",
    "* Runs a suite of **sanity checks**:\n",
    "\n",
    "  * shape, dtype, normalization range\n",
    "  * missing/invalid values (NaN/Inf)\n",
    "  * temporal consistency across `X`, `mask`, and `timestamps`\n",
    "  * velocity spikes and bone length jitter\n",
    "  * keypoint coverage and visibility per frame\n",
    "* Verifies compatibility with:\n",
    "\n",
    "  * **Transformer model**: expects `[T,156]` float32 keypoints + `[T,78]` bool mask\n",
    "  * **IV3-GRU model**: expects `[T,2048]` float32 features, temporally aligned with `X`\n",
    "* Produces structured outputs:\n",
    "\n",
    "  * `npz_audit_report.csv` → compact tabular summary for all files\n",
    "  * `npz_audit_report.jsonl` → detailed JSON-per-file for debugging\n",
    "  * `npz_audit_summary.md` → human-readable overview with issue counts\n",
    "* (Optional) Creates MP4 animations of keypoints for quick QA.\n",
    "  *(can be disabled with `make_videos=False` or `--no-video`)*\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂 Expected Directory Layout\n",
    "\n",
    "The script assumes the following structure:\n",
    "\n",
    "```\n",
    "flsr-transformer-vs-iv3gru/\n",
    "│\n",
    "├── data/\n",
    "│   ├── processed/\n",
    "│   │   └── keypoints_all/        # input .npz files (2130 clips)\n",
    "│   ├── raw/                      # raw data (not used here)\n",
    "│   └── reports/                  # outputs written here\n",
    "│       ├── npz_audit_report.csv\n",
    "│       ├── npz_audit_report.jsonl\n",
    "│       ├── npz_audit_summary.md\n",
    "│       └── (optional sample_animations/ if enabled)\n",
    "│\n",
    "├── notebooks/\n",
    "│   └── validate_npz_enhanced.ipynb  # Jupyter notebook version\n",
    "│\n",
    "├── preprocessing/\n",
    "│   └── validate_npz.py              # legacy checker (optional import)\n",
    "│\n",
    "└── models/ ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ How it works\n",
    "\n",
    "1. **Select inputs**\n",
    "\n",
    "   * Single file:\n",
    "\n",
    "     ```bash\n",
    "     python validate_npz_enhanced.py --file data/processed/keypoints_all/15.npz\n",
    "     ```\n",
    "   * Entire directory:\n",
    "\n",
    "     ```bash\n",
    "     python validate_npz_enhanced.py --dir data/processed/keypoints_all --out data/reports\n",
    "     ```\n",
    "\n",
    "2. **Run validations**\n",
    "\n",
    "   * Script loads each `.npz` and applies `basic_sanity`, `transformer_ready`, `iv3_gru_ready`.\n",
    "   * Collects per-file metrics: coverage %, velocity spikes, bone jitter, out-of-range values.\n",
    "\n",
    "3. **Write outputs**\n",
    "\n",
    "   * Streaming writes to `npz_audit_report.jsonl` and `npz_audit_report.csv` while iterating.\n",
    "   * At the end, compiles issue counts into `npz_audit_summary.md`.\n",
    "\n",
    "4. **(Optional) Animations**\n",
    "\n",
    "   * If enabled, saves keypoint visualizations under `data/reports/sample_animations/`.\n",
    "   * Skippable for large datasets to save space.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Usage Options\n",
    "\n",
    "* Skip animations:\n",
    "\n",
    "  ```bash\n",
    "  python validate_npz_enhanced.py --dir data/processed/keypoints_all --out data/reports --no-video\n",
    "  ```\n",
    "* Notebook workflow: open `notebooks/validate_npz_enhanced.ipynb` and run all cells.\n",
    "* Reports are ready for sharing via Git; logs are optional and can be ignored.\n",
    "\n",
    "---\n",
    "\n",
    "## 📄 Outputs\n",
    "\n",
    "* **CSV**: quick-glance metrics across all files\n",
    "* **JSONL**: detailed raw audit records\n",
    "* **MD summary**: counts of issues (ready to share with team)\n",
    "* *(optional)* per-file animations\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Common Issues & Fixes\n",
    "\n",
    "When reviewing the CSV/MD reports, here’s how to interpret the most common flags:\n",
    "\n",
    "| Field / Issue                  | What it means                                                                 | Fix / Next Step                                                                  |\n",
    "| ------------------------------ | ----------------------------------------------------------------------------- | -------------------------------------------------------------------------------- |\n",
    "| `X_out_of_range > 0`           | Some keypoint coordinates fall outside `[0,1]`. Data not normalized properly. | Re-check preprocessing step that scales coordinates.                             |\n",
    "| `X_has_nan` / `X_has_inf`      | Non-finite values in keypoints.                                               | Identify affected clips, fix preprocessing, or drop those samples.               |\n",
    "| `timestamps_monotonic = False` | Frame timestamps go backward / are inconsistent.                              | Check data loader or export step that writes `timestamps_ms`.                    |\n",
    "| `temporal_consistent = False`  | `X`, `mask`, and `timestamps` don’t all have the same frame count.            | Ensure preprocessing writes aligned arrays per clip.                             |\n",
    "| `low_vis_frames > 0`           | Some frames have <10 visible keypoints (likely detector dropout).             | Consider filtering frames or interpolating missing detections.                   |\n",
    "| `vel_spike_frames > 0`         | Sudden pose “teleports” between frames.                                       | Re-check tracking or smooth post-process with filtering.                         |\n",
    "| `bone_len_cv_pct` high (>15%)  | Bone lengths vary too much → possible identity swaps or jitter.               | Apply skeleton smoothing or inspect detector/tracker quality.                    |\n",
    "| Transformer issues             | Shape/dtype mismatch or missing mask/timestamps.                              | Adjust preprocessing pipeline to meet `[T,156]` float32 + `[T,78]` bool spec.    |\n",
    "| IV3-GRU issues                 | Missing/invalid `X2048` features.                                             | Ensure InceptionV3 features were extracted; re-run feature extraction if needed. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15242c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 1: Setup & Imports ---\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple\n",
    "from datetime import datetime\n",
    "import os, sys, json, logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FFMpegWriter, FuncAnimation\n",
    "\n",
    "# Make repo root importable (assuming this notebook lives in /notebooks)\n",
    "ROOT = Path.cwd().parent\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))\n",
    "\n",
    "# Try optional validator from restructured preprocessing module (safe if missing)\n",
    "try:\n",
    "    from preprocessing.utils.validate_npz import validate_npz_file  # may be None if not present\n",
    "except Exception:\n",
    "    validate_npz_file = None\n",
    "\n",
    "print(\"Project ROOT:\", ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f05171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 2: Config, Topology, Logging Helpers (no file logs) ---\n",
    "\n",
    "# 78 keypoints (25 body, 21 LH, 21 RH, 11 face) => X shape [T, 156], mask [T, 78]\n",
    "SPLITS = {\n",
    "    \"pose\": list(range(0, 25)),\n",
    "    \"left_hand\": list(range(25, 46)),\n",
    "    \"right_hand\": list(range(46, 67)),\n",
    "    \"face\": list(range(67, 78)),\n",
    "}\n",
    "\n",
    "# Lightweight bones for stability checks (indices must exist)\n",
    "BONES = [\n",
    "    (5,7), (7,9), (6,8), (8,10),\n",
    "    (11,13), (13,15), (12,14), (14,16),\n",
    "    (5,6), (5,11), (6,12), (11,12),\n",
    "]\n",
    "\n",
    "DEFAULTS = dict(\n",
    "    expect_x_dim=156,\n",
    "    expect_mask_dim=78,\n",
    "    transformer_dtype=np.float32,\n",
    "    iv3_dim=2048,\n",
    "    iv3_dtype=np.float32,\n",
    "    x_norm_range=(0.0, 1.0),\n",
    "    low_vis_threshold=10,       # frames with <10 visible kpts flagged\n",
    "    vel_spike_sigma=3.0,        # spike if > median + 3*std\n",
    "    bone_cv_threshold_2d=0.15,  # CV > 15% => jitter / identity issues\n",
    ")\n",
    "\n",
    "# --- Logging helpers: console only, no log files ---\n",
    "def setup_logger(log_dir: Path, name=\"npz_audit\") -> logging.Logger:\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.handlers.clear()\n",
    "    sh = logging.StreamHandler(sys.stdout)\n",
    "    fmt = logging.Formatter(\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "    sh.setFormatter(fmt)\n",
    "    logger.addHandler(sh)\n",
    "    return logger\n",
    "\n",
    "def perfile_logger(base_dir: Path, stem: str) -> logging.Logger:\n",
    "    # simplified: return the root logger so we don’t create per-file logs\n",
    "    return logging.getLogger(\"npz_audit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab94eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 3: Core QA Utilities ---\n",
    "\n",
    "def is_monotonic_nondec(arr: np.ndarray) -> bool:\n",
    "    return arr.size < 2 or np.all(arr[1:] >= arr[:-1])\n",
    "\n",
    "def coverage_stats(mask: np.ndarray) -> Dict[str, Any]:\n",
    "    if mask is None or mask.size == 0:\n",
    "        return dict(has_mask=False, coverage_pct=0.0, min_visible=0, max_visible=0, low_vis_frames=0)\n",
    "    vis_counts = mask.sum(axis=1)\n",
    "    return dict(\n",
    "        has_mask=True,\n",
    "        coverage_pct=float(mask.mean() * 100.0),\n",
    "        min_visible=int(vis_counts.min()),\n",
    "        max_visible=int(vis_counts.max()),\n",
    "        low_vis_frames=int((vis_counts < DEFAULTS[\"low_vis_threshold\"]).sum()),\n",
    "    )\n",
    "\n",
    "def velocity_metrics(X: np.ndarray) -> Dict[str, Any]:\n",
    "    if X.shape[0] < 2:\n",
    "        return dict(vel_p95=0.0, vel_max=0.0, vel_spike_frames=0)\n",
    "    xy = X.reshape(X.shape[0], -1, 2)\n",
    "    v = np.linalg.norm(np.diff(xy, axis=0), axis=2)  # (T-1, J)\n",
    "    p95 = float(np.nanpercentile(v, 95))\n",
    "    vmax = float(np.nanmax(v))\n",
    "    frame_max = np.nanmax(v, axis=1)\n",
    "    med, std = float(np.nanmedian(frame_max)), float(np.nanstd(frame_max))\n",
    "    spikes = int(np.sum(frame_max > (med + DEFAULTS[\"vel_spike_sigma\"] * std)))\n",
    "    return dict(vel_p95=p95, vel_max=vmax, vel_spike_frames=spikes)\n",
    "\n",
    "def bone_length_cv(X: np.ndarray) -> Dict[str, Any]:\n",
    "    T = X.shape[0]\n",
    "    if T == 0 or not BONES:\n",
    "        return dict(bone_len_cv_pct=0.0, bone_outlier_frames=0)\n",
    "    xy = X.reshape(T, -1, 2)\n",
    "    BL = []\n",
    "    for a, b in BONES:\n",
    "        if max(a, b) >= xy.shape[1]:\n",
    "            continue\n",
    "        BL.append(np.linalg.norm(xy[:, a, :] - xy[:, b, :], axis=1))\n",
    "    if not BL:\n",
    "        return dict(bone_len_cv_pct=0.0, bone_outlier_frames=0)\n",
    "    BL = np.stack(BL, axis=1)  # (T, B)\n",
    "    cv = float(np.nanstd(BL) / (np.nanmean(BL) + 1e-9))\n",
    "    z = (BL - np.nanmean(BL, axis=0)) / (np.nanstd(BL, axis=0) + 1e-9)\n",
    "    outlier_frames = int(np.any(np.abs(z) > 3.5, axis=1).sum())\n",
    "    return dict(bone_len_cv_pct=cv * 100.0, bone_outlier_frames=outlier_frames)\n",
    "\n",
    "def basic_sanity(X: np.ndarray, mask: np.ndarray, timestamps: np.ndarray) -> Dict[str, Any]:\n",
    "    report: Dict[str, Any] = {}\n",
    "    # shapes\n",
    "    report[\"T\"] = int(X.shape[0])\n",
    "    report[\"X_shape\"] = list(X.shape)\n",
    "    report[\"mask_shape\"] = list(mask.shape) if mask is not None else None\n",
    "    report[\"ts_shape\"] = list(timestamps.shape) if timestamps is not None else None\n",
    "    # dtypes\n",
    "    report[\"X_dtype\"] = str(X.dtype)\n",
    "    report[\"mask_dtype\"] = str(mask.dtype) if mask is not None else None\n",
    "    report[\"ts_dtype\"] = str(timestamps.dtype) if timestamps is not None else None\n",
    "    # finiteness\n",
    "    report[\"X_has_nan\"] = bool(np.isnan(X).any())\n",
    "    report[\"X_has_inf\"] = bool(np.isinf(X).any())\n",
    "    # range (normalized)\n",
    "    lo, hi = DEFAULTS[\"x_norm_range\"]\n",
    "    report[\"X_out_of_range\"] = int(((X < lo) | (X > hi)).sum())\n",
    "    # timestamps\n",
    "    if timestamps is not None and timestamps.size > 1:\n",
    "        report[\"timestamps_monotonic\"] = bool(is_monotonic_nondec(timestamps))\n",
    "        report[\"duration_ms\"] = int(timestamps[-1] - timestamps[0])\n",
    "    else:\n",
    "        report[\"timestamps_monotonic\"] = True\n",
    "        report[\"duration_ms\"] = 0\n",
    "    # temporal consistency\n",
    "    t_ok = [X.shape[0]]\n",
    "    if mask is not None: t_ok.append(mask.shape[0])\n",
    "    if timestamps is not None: t_ok.append(timestamps.shape[0])\n",
    "    report[\"temporal_consistent\"] = (len(set(t_ok)) == 1)\n",
    "    # mask coverage + kinematics\n",
    "    report.update(coverage_stats(mask))\n",
    "    report.update(velocity_metrics(X))\n",
    "    report.update(bone_length_cv(X))\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f9a394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 4: Model Validators ---\n",
    "\n",
    "def transformer_ready(X: np.ndarray, mask: np.ndarray, timestamps: np.ndarray) -> Tuple[bool, List[str]]:\n",
    "    issues = []\n",
    "    if not (X.ndim == 2 and X.shape[1] == DEFAULTS[\"expect_x_dim\"] and X.dtype == DEFAULTS[\"transformer_dtype\"]):\n",
    "        issues.append(\"X shape/dtype incorrect (expected [T,156] float32)\")\n",
    "    if mask is None or not (mask.ndim == 2 and mask.shape[1] == DEFAULTS[\"expect_mask_dim\"] and mask.dtype == np.bool_):\n",
    "        issues.append(\"mask shape/dtype incorrect (expected [T,78] bool)\")\n",
    "    if timestamps is None or not (timestamps.ndim == 1 and timestamps.dtype == np.int64):\n",
    "        issues.append(\"timestamps incorrect (expected [T] int64)\")\n",
    "    if X.shape[0] != (mask.shape[0] if mask is not None else -1) or X.shape[0] != (timestamps.shape[0] if timestamps is not None else -1):\n",
    "        issues.append(\"temporal dimension mismatch among X/mask/timestamps\")\n",
    "    if np.isnan(X).any() or np.isinf(X).any():\n",
    "        issues.append(\"non-finite values in X\")\n",
    "    lo, hi = DEFAULTS[\"x_norm_range\"]\n",
    "    if ((X < lo) | (X > hi)).any():\n",
    "        issues.append(\"X outside [0,1] range (expected normalized)\")\n",
    "    if timestamps is not None and timestamps.size > 1 and not is_monotonic_nondec(timestamps):\n",
    "        issues.append(\"timestamps not monotonic\")\n",
    "    return (len(issues) == 0), issues\n",
    "\n",
    "def iv3_gru_ready(X: np.ndarray, X2048: np.ndarray | None) -> Tuple[bool, List[str]]:\n",
    "    issues = []\n",
    "    if X2048 is None:\n",
    "        return False, [\"X2048 missing (required for IV3-GRU)\"]\n",
    "    if not (X2048.ndim == 2 and X2048.shape[1] == DEFAULTS[\"iv3_dim\"] and X2048.dtype == DEFAULTS[\"iv3_dtype\"]):\n",
    "        issues.append(\"X2048 shape/dtype incorrect (expected [T,2048] float32)\")\n",
    "    if X2048.shape[0] != X.shape[0]:\n",
    "        issues.append(f\"X2048 temporal mismatch: X({X.shape[0]}) vs X2048({X2048.shape[0]})\")\n",
    "    if np.isnan(X2048).any() or np.isinf(X2048).any():\n",
    "        issues.append(\"non-finite values in X2048\")\n",
    "    return (len(issues) == 0), issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7678e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 5: Animation Helpers ---\n",
    "\n",
    "def save_animation_mp4(\n",
    "    X: np.ndarray,\n",
    "    mask: np.ndarray,\n",
    "    timestamps: np.ndarray,\n",
    "    out_path: Path,\n",
    "    split_map=SPLITS,\n",
    "    fps=30,\n",
    "    dpi=110,\n",
    "    title_prefix=\"Keypoints\",\n",
    "    add_info=True,\n",
    "):\n",
    "    T = X.shape[0]\n",
    "    xy = X.reshape(T, -1, 2)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.set_xlim(0, 1); ax.set_ylim(1, 0)\n",
    "    ax.set_xlabel(\"X (normalized)\"); ax.set_ylabel(\"Y (normalized)\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    scatters = {name: ax.scatter([], [], s=30, label=name) for name in split_map}\n",
    "    inv_scatter = ax.scatter([], [], facecolors='none', edgecolors='k', s=60, label='invisible')\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "    def update(i):\n",
    "        xi, yi = xy[i, :, 0], xy[i, :, 1]\n",
    "        vis = mask[i] if mask is not None and i < mask.shape[0] else np.ones(len(xi), dtype=bool)\n",
    "        for name, idxs in split_map.items():\n",
    "            pts = np.column_stack([xi[idxs], yi[idxs]]) if max(idxs) < len(xi) else np.empty((0, 2))\n",
    "            scatters[name].set_offsets(pts)\n",
    "        inv = np.where(~vis)[0]\n",
    "        inv_pts = np.column_stack([xi[inv], yi[inv]]) if inv.size else np.empty((0, 2))\n",
    "        inv_scatter.set_offsets(inv_pts)\n",
    "        if add_info:\n",
    "            label = [f\"Frame {i+1}/{T}\"]\n",
    "            if timestamps is not None and timestamps.size > i:\n",
    "                label.append(f\"t={timestamps[i]/1000:.2f}s\")\n",
    "            ax.set_title(f\"{title_prefix} | \" + \" | \".join(label))\n",
    "        return list(scatters.values()) + [inv_scatter]\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=T, interval=1000//max(1, fps), blit=False)\n",
    "    writer = FFMpegWriter(fps=fps, metadata=dict(artist=\"npz_audit\"), bitrate=1800)\n",
    "    ani.save(str(out_path), writer=writer, dpi=dpi)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53a8cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 6: Single-File Audit ---\n",
    "\n",
    "def audit_npz(npz_path: Path, out_root: Path, save_viz: bool = True) -> Dict[str, Any]:\n",
    "    data = np.load(npz_path, allow_pickle=True)\n",
    "\n",
    "    def get(name, default=None):\n",
    "        return data[name] if name in data.files else default\n",
    "\n",
    "    X = get(\"X\")\n",
    "    mask = get(\"mask\")\n",
    "    ts = get(\"timestamps_ms\")\n",
    "    X2048 = get(\"X2048\", None)\n",
    "\n",
    "    # Parse meta safely\n",
    "    meta_raw = get(\"meta\", None)\n",
    "    meta = None\n",
    "    if meta_raw is not None:\n",
    "        try:\n",
    "            if hasattr(meta_raw, \"item\"):\n",
    "                meta_raw = meta_raw.item()\n",
    "            if isinstance(meta_raw, bytes):\n",
    "                meta_raw = meta_raw.decode(\"utf-8\")\n",
    "            meta = json.loads(meta_raw) if isinstance(meta_raw, str) else None\n",
    "        except Exception:\n",
    "            meta = {\"_error\": \"bad_meta_json\"}\n",
    "\n",
    "    # Minimal required keys\n",
    "    if X is None or mask is None or ts is None:\n",
    "        return dict(file=str(npz_path), error=\"missing_required_keys\", present=list(data.files))\n",
    "\n",
    "    # Basic sanity\n",
    "    sanity = basic_sanity(X, mask, ts)\n",
    "\n",
    "    # Model readiness\n",
    "    t_ok, t_issues = transformer_ready(X, mask, ts)\n",
    "    i_ok, i_issues = iv3_gru_ready(X, X2048)\n",
    "\n",
    "    # Optional: legacy comprehensive validator\n",
    "    legacy = []\n",
    "    if validate_npz_file is not None:\n",
    "        try:\n",
    "            legacy = validate_npz_file(\n",
    "                str(npz_path),\n",
    "                require_x2048=True,\n",
    "                check_parquet=False,\n",
    "                check_transformer=True,\n",
    "                check_iv3=True,\n",
    "            ) or []\n",
    "        except Exception as e:\n",
    "            legacy = [f\"validate_npz_file_error:{e}\"]\n",
    "\n",
    "    # Animations\n",
    "    # anim_dir = out_root / \"sample_animations\"\n",
    "    # if save_viz:\n",
    "    #    try:\n",
    "    #        save_animation_mp4(X, mask, ts, anim_dir / f\"{npz_path.stem}_preview.mp4\",  fps=30, dpi=100, title_prefix=\"Preview\",  add_info=False)\n",
    "    #        save_animation_mp4(X, mask, ts, anim_dir / f\"{npz_path.stem}_detailed.mp4\", fps=15, dpi=150, title_prefix=\"Detailed\", add_info=True)\n",
    "    #       save_animation_mp4(X, mask, ts, anim_dir / f\"{npz_path.stem}_slowmo.mp4\",  fps=10, dpi=120, title_prefix=\"SlowMo\",   add_info=True)\n",
    "    #    except Exception as e:\n",
    "    #        sanity[\"animation_error\"] = str(e)\n",
    "\n",
    "    rec: Dict[str, Any] = dict(\n",
    "        file=str(npz_path),\n",
    "        present=list(data.files),\n",
    "        meta_summary=(list(meta.keys())[:8] if isinstance(meta, dict) else None),\n",
    "        **sanity,\n",
    "        transformer_ready=t_ok,\n",
    "        transformer_issues=t_issues,\n",
    "        iv3_ready=i_ok,\n",
    "        iv3_issues=i_issues,\n",
    "        legacy_issues=legacy,\n",
    "    )\n",
    "\n",
    "    if X2048 is not None and X2048.size:\n",
    "        rec.update(\n",
    "            dict(\n",
    "                X2048_shape=list(X2048.shape),\n",
    "                X2048_dtype=str(X2048.dtype),\n",
    "                X2048_min=float(np.nanmin(X2048)),\n",
    "                X2048_max=float(np.nanmax(X2048)),\n",
    "                X2048_mean=float(np.nanmean(X2048)),\n",
    "                X2048_std=float(np.nanstd(X2048)),\n",
    "            )\n",
    "        )\n",
    "    return rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56ab7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 7: Batch Runner + Report Writers ---\n",
    "\n",
    "def run_audit(npz_dir: Path | str | None,\n",
    "              npz_file: Path | str | None,\n",
    "              out_root: Path | str,\n",
    "              make_videos: bool = True):\n",
    "    # Normalize paths (robust to strings)\n",
    "    if isinstance(npz_dir, (str, os.PathLike)) and npz_dir is not None:\n",
    "        npz_dir = Path(npz_dir)\n",
    "    if isinstance(npz_file, (str, os.PathLike)) and npz_file is not None:\n",
    "        npz_file = Path(npz_file)\n",
    "    if isinstance(out_root, (str, os.PathLike)):\n",
    "        out_root = Path(out_root)\n",
    "\n",
    "    logs_dir = out_root / \"logs\"\n",
    "    reports_dir = out_root\n",
    "    logger = setup_logger(logs_dir)\n",
    "    records: List[Dict[str, Any]] = []\n",
    "\n",
    "    # Select targets\n",
    "    if npz_file:\n",
    "        targets = [npz_file]\n",
    "    elif npz_dir:\n",
    "        if not npz_dir.exists():\n",
    "            logger.error(f\"Directory not found: {npz_dir}\")\n",
    "            return\n",
    "        targets = sorted(npz_dir.glob(\"*.npz\"))\n",
    "    else:\n",
    "        logger.error(\"Provide either npz_dir or npz_file\")\n",
    "        return\n",
    "\n",
    "    if not targets:\n",
    "        logger.warning(f\"No .npz files found in {npz_dir if npz_dir else npz_file}\")\n",
    "        return\n",
    "\n",
    "    logger.info(f\"Found {len(targets)} target file(s)\")\n",
    "\n",
    "    for p in targets:\n",
    "        flog = perfile_logger(logs_dir, p.stem)\n",
    "        flog.info(f\"Auditing {p.name}\")\n",
    "        try:\n",
    "            rec = audit_npz(p, out_root, save_viz=make_videos)\n",
    "            records.append(rec)\n",
    "\n",
    "            verdict = []\n",
    "            verdict.append(\"Transformer: OK\" if rec[\"transformer_ready\"] else f\"Transformer: FAIL ({len(rec['transformer_issues'])} issues)\")\n",
    "            verdict.append(\"IV3-GRU: OK\" if rec[\"iv3_ready\"] else f\"IV3-GRU: FAIL ({len(rec['iv3_issues'])} issues)\")\n",
    "            flog.info(\" | \".join(verdict))\n",
    "\n",
    "            flog.info(\n",
    "                \"coverage={:.1f}%  low_vis_frames={}  vel_p95={:.3f}  bone_cv%={:.1f}  X_out_of_range={}  X_has_nan={}  X_has_inf={}\".format(\n",
    "                    rec.get(\"coverage_pct\", 0.0),\n",
    "                    rec.get(\"low_vis_frames\", 0),\n",
    "                    rec.get(\"vel_p95\", 0.0),\n",
    "                    rec.get(\"bone_len_cv_pct\", 0.0),\n",
    "                    rec.get(\"X_out_of_range\", 0),\n",
    "                    rec.get(\"X_has_nan\"),\n",
    "                    rec.get(\"X_has_inf\"),\n",
    "                )\n",
    "            )\n",
    "        except Exception as e:\n",
    "            flog.exception(f\"Failed {p.name}: {e}\")\n",
    "            records.append(dict(file=str(p), error=str(e)))\n",
    "\n",
    "    # Write JSONL and CSV\n",
    "    reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "    jsonl = reports_dir / \"npz_audit_report.jsonl\"\n",
    "    csv   = reports_dir / \"npz_audit_report.csv\"\n",
    "\n",
    "    with open(jsonl, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in records:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    import csv as _csv\n",
    "    csv_fields = [\n",
    "        \"file\",\"T\",\"coverage_pct\",\"low_vis_frames\",\"vel_p95\",\"vel_max\",\n",
    "        \"vel_spike_frames\",\"bone_len_cv_pct\",\"bone_outlier_frames\",\"X_out_of_range\",\n",
    "        \"X_has_nan\",\"X_has_inf\",\"timestamps_monotonic\",\"temporal_consistent\",\n",
    "        \"transformer_ready\",\"iv3_ready\"\n",
    "    ]\n",
    "    with open(csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = _csv.DictWriter(f, fieldnames=csv_fields)\n",
    "        w.writeheader()\n",
    "        for r in records:\n",
    "            w.writerow({k: r.get(k) for k in csv_fields})\n",
    "\n",
    "    # Markdown summary\n",
    "    md = reports_dir / \"npz_audit_summary.md\"\n",
    "    total = len(records)\n",
    "    t_pass = sum(1 for r in records if r.get(\"transformer_ready\"))\n",
    "    i_pass = sum(1 for r in records if r.get(\"iv3_ready\"))\n",
    "    with open(md, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"# NPZ Audit Summary ({datetime.now().isoformat(timespec='seconds')})\\n\\n\")\n",
    "        f.write(f\"- Files audited: **{total}**\\n\")\n",
    "        f.write(f\"- Transformer ready: **{t_pass}/{total}**\\n\")\n",
    "        f.write(f\"- IV3-GRU ready: **{i_pass}/{total}**\\n\\n\")\n",
    "        f.write(\"## Top issues (counts)\\n\")\n",
    "        issues = {}\n",
    "        for r in records:\n",
    "            for tag in r.get(\"transformer_issues\", []) + r.get(\"iv3_issues\", []) + r.get(\"legacy_issues\", []):\n",
    "                issues[tag] = issues.get(tag, 0) + 1\n",
    "        for k, v in sorted(issues.items(), key=lambda kv: (-kv[1], kv[0]))[:20]:\n",
    "            f.write(f\"- {k}: {v}\\n\")\n",
    "\n",
    "    logger.info(f\"Reports written:\\n- {jsonl}\\n- {csv}\\n- {md}\\nLogs:\\n- {logs_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8544a7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 8: Run Audit (Notebook-Friendly Defaults) ---\n",
    "\n",
    "# Input folder of .npz files\n",
    "NPZ_DIR = ROOT / \"data\" / \"processed\" / \"keypoints_all\"\n",
    "# Output folder for reports + logs + animations\n",
    "OUT_DIR = ROOT / \"data\" / \"reports\"\n",
    "\n",
    "print(\"NPZ_DIR:\", NPZ_DIR)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n",
    "\n",
    "# Run on a folder (set make_videos=False if ffmpeg is not available)\n",
    "run_audit(npz_dir=NPZ_DIR, npz_file=None, out_root=OUT_DIR, make_videos=False)\n",
    "\n",
    "# Example: run on a single file\n",
    "# run_audit(npz_dir=None, npz_file=NPZ_DIR / \"15.npz\", out_root=OUT_DIR, make_videos=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
