{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Notebook\n",
    "\n",
    "This notebook provides comprehensive evaluation of trained FSLR models (Transformer and IV3-GRU) on test data.\n",
    "\n",
    "## What this notebook does:\n",
    "1. **Loads trained models** from checkpoints (both Transformer and IV3-GRU)\n",
    "2. **Evaluates on test data** with comprehensive metrics\n",
    "3. **Visualizes results** with interactive plots and confusion matrices\n",
    "4. **Analyzes errors** and misclassification patterns\n",
    "5. **Compares models** side-by-side (optional)\n",
    "\n",
    "## Prerequisites:\n",
    "- Trained model checkpoints in `data/processed/`\n",
    "- Test data prepared in the same format as training data\n",
    "- Test labels CSV with columns: `file,gloss,cat`\n",
    "\n",
    "## Usage:\n",
    "1. Update the configuration cell with your model and data paths\n",
    "2. Run cells sequentially\n",
    "3. Explore results interactively\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "CWD = Path.cwd()\n",
    "ROOT = CWD.parent if CWD.name == 'notebooks' else CWD\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))\n",
    "\n",
    "# Core imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score, top_k_accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score, matthews_corrcoef\n",
    "from scipy import stats\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.offline as pyo\n",
    "\n",
    "# Project imports\n",
    "from models.transformer import SignTransformer\n",
    "from models.iv3_gru import InceptionV3GRU\n",
    "from training.utils import FSLDataset, evaluate\n",
    "from training.train import FSLKeypointFileDataset, FSLFeatureFileDataset, collate_keypoints_with_padding, collate_features_with_padding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"Project root: {ROOT}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Update these paths to point to your trained models and test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Update these paths for your setup\n",
    "CONFIG = {\n",
    "    # Model selection\n",
    "    'model_type': 'transformer',  # 'transformer' or 'iv3_gru'\n",
    "    'model_path': 'data/processed/SignTransformer_best.pt',  # Path to checkpoint\n",
    "    \n",
    "    # Test data paths\n",
    "    'test_data_path': 'data/processed/test_keypoints',  # Directory with test .npz files\n",
    "    'test_labels_path': 'data/processed/test_labels.csv',  # CSV with test labels\n",
    "    \n",
    "    # Model parameters (should match training)\n",
    "    'num_gloss': 105,\n",
    "    'num_cat': 10,\n",
    "    'hidden1': 16,  # For IV3-GRU\n",
    "    'hidden2': 12,  # For IV3-GRU\n",
    "    'dropout': 0.3,  # For IV3-GRU\n",
    "    \n",
    "    # Evaluation parameters\n",
    "    'batch_size': 32,\n",
    "    'num_workers': 0,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \n",
    "    # Data format (for IV3-GRU)\n",
    "    'feature_key': 'X2048',  # Key in .npz files for features\n",
    "    'kp_key': 'X',  # Key in .npz files for keypoints\n",
    "    \n",
    "    # Visualization\n",
    "    'show_plots': True,\n",
    "    'save_plots': False,\n",
    "    'plot_dpi': 100,\n",
    "    \n",
    "    # Statistical analysis\n",
    "    'confidence_level': 0.95,\n",
    "    'bootstrap_samples': 1000,\n",
    "}\n",
    "\n",
    "def validate_configuration(config):\n",
    "    \"\"\"Validate configuration and check file paths\"\"\"\n",
    "    print(\"Configuration:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Validate paths\n",
    "    model_path = ROOT / config['model_path']\n",
    "    test_data_path = ROOT / config['test_data_path']\n",
    "    test_labels_path = ROOT / config['test_labels_path']\n",
    "    \n",
    "    print(f\"\\nPath validation:\")\n",
    "    print(f\"  Model exists: {model_path.exists()}\")\n",
    "    print(f\"  Test data exists: {test_data_path.exists()}\")\n",
    "    print(f\"  Test labels exist: {test_labels_path.exists()}\")\n",
    "    \n",
    "    missing_paths = []\n",
    "    if not model_path.exists():\n",
    "        missing_paths.append(\"Model checkpoint\")\n",
    "    if not test_data_path.exists():\n",
    "        missing_paths.append(\"Test data directory\")\n",
    "    if not test_labels_path.exists():\n",
    "        missing_paths.append(\"Test labels file\")\n",
    "    \n",
    "    if missing_paths:\n",
    "        print(f\"WARNING: Missing required files: {', '.join(missing_paths)}\")\n",
    "        print(\"Please update the configuration above with correct paths.\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Display and validate configuration\n",
    "config_valid = validate_configuration(CONFIG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading\n",
    "\n",
    "Load the trained model from checkpoint and prepare for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_model(model_type, model_path, device, **model_kwargs):\n    \"\"\"Load trained model from checkpoint with comprehensive error handling\"\"\"\n    print(f\"Loading {model_type} model from {model_path}\")\n    \n    try:\n        # Check if file exists first\n        if not model_path.exists():\n            raise FileNotFoundError(f\"Model checkpoint not found: {model_path}\")\n        \n        # Load checkpoint\n        checkpoint = torch.load(model_path, map_location=device)\n        print(f\"Checkpoint loaded (epoch {checkpoint.get('epoch', 'unknown')})\")\n        \n        # Validate checkpoint structure\n        if 'model' not in checkpoint:\n            raise KeyError(\"Checkpoint missing 'model' key - invalid checkpoint format\")\n        \n        # Create model architecture\n        if model_type == 'transformer':\n            model = SignTransformer(\n                num_gloss=model_kwargs['num_gloss'],\n                num_cat=model_kwargs['num_cat']\n            )\n        elif model_type == 'iv3_gru':\n            model = InceptionV3GRU(\n                num_gloss=model_kwargs['num_gloss'],\n                num_cat=model_kwargs['num_cat'],\n                hidden1=model_kwargs['hidden1'],\n                hidden2=model_kwargs['hidden2'],\n                dropout=model_kwargs['dropout'],\n                pretrained_backbone=True,\n                freeze_backbone=True\n            )\n        else:\n            raise ValueError(f\"Unknown model type: {model_type}. Expected 'transformer' or 'iv3_gru'\")\n        \n        # Load state dict with error checking\n        try:\n            model.load_state_dict(checkpoint['model'])\n        except RuntimeError as e:\n            print(f\"WARNING: Model state dict mismatch: {e}\")\n            print(\"Attempting to load with strict=False...\")\n            model.load_state_dict(checkpoint['model'], strict=False)\n        \n        model.to(device)\n        model.eval()\n        \n        # Get model info\n        total_params = sum(p.numel() for p in model.parameters())\n        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n        \n        print(f\"Model loaded successfully!\")\n        print(f\"  - Model type: {model.__class__.__name__}\")\n        print(f\"  - Total parameters: {total_params:,}\")\n        print(f\"  - Trainable parameters: {trainable_params:,}\")\n        print(f\"  - Model size: {total_params * 4 / 1024 / 1024:.1f} MB\")\n        print(f\"  - Best validation metric: {checkpoint.get('best_metric', 'unknown')}\")\n        print(f\"  - Device: {device}\")\n        \n        # Validate model is in eval mode\n        if model.training:\n            print(\"WARNING: Model is in training mode, switching to eval mode\")\n            model.eval()\n        \n        return model, checkpoint\n        \n    except FileNotFoundError:\n        print(f\"ERROR: Model file not found at {model_path}\")\n        print(\"Please check the model_path in the configuration section\")\n        raise\n    except KeyError as e:\n        print(f\"ERROR: Invalid checkpoint format - missing key: {e}\")\n        print(\"The checkpoint file may be corrupted or from an incompatible version\")\n        raise\n    except RuntimeError as e:\n        print(f\"ERROR: Model architecture mismatch: {e}\")\n        print(\"The checkpoint may have been saved with different model parameters\")\n        raise\n    except Exception as e:\n        print(f\"ERROR: Unexpected error loading model: {type(e).__name__}: {e}\")\n        raise\n\n# Load the model with better error handling\nif config_valid:\n    try:\n        model, checkpoint = load_model(\n            model_type=CONFIG['model_type'],\n            model_path=ROOT / CONFIG['model_path'],\n            device=CONFIG['device'],\n            **{k: v for k, v in CONFIG.items() if k in ['num_gloss', 'num_cat', 'hidden1', 'hidden2', 'dropout']}\n        )\n        print(\"Model loading successful - ready for evaluation\")\n    except Exception as e:\n        print(f\"Failed to load model: {e}\")\n        print(\"Please check your configuration and model file\")\n        model = None\n        checkpoint = None\nelse:\n    print(\"Skipping model loading due to configuration validation failure.\")\n    model = None\n    checkpoint = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data Loading\n",
    "\n",
    "Load and prepare the test dataset for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_test_data(config):\n    \"\"\"Load test dataset and create dataloader with comprehensive validation\"\"\"\n    print(f\"Loading test data from {config['test_data_path']}\")\n    \n    try:\n        # Validate paths exist\n        test_data_path = ROOT / config['test_data_path']\n        test_labels_path = ROOT / config['test_labels_path']\n        \n        if not test_data_path.exists():\n            raise FileNotFoundError(f\"Test data directory not found: {test_data_path}\")\n        if not test_labels_path.exists():\n            raise FileNotFoundError(f\"Test labels file not found: {test_labels_path}\")\n        \n        # Load labels\n        labels_df = pd.read_csv(test_labels_path)\n        print(f\"Loaded {len(labels_df)} test samples\")\n        \n        # Validate labels format\n        required_columns = ['file', 'gloss', 'cat']\n        missing_columns = [col for col in required_columns if col not in labels_df.columns]\n        if missing_columns:\n            raise ValueError(f\"Missing required columns in labels: {missing_columns}\")\n        \n        # Check for missing values\n        if labels_df.isnull().any().any():\n            print(\"WARNING: Found missing values in labels. Filling with defaults.\")\n            labels_df = labels_df.fillna({'gloss': 0, 'cat': 0})\n        \n        # Validate class ranges\n        max_gloss = labels_df['gloss'].max()\n        max_cat = labels_df['cat'].max()\n        if max_gloss >= config['num_gloss']:\n            raise ValueError(f\"Gloss label {max_gloss} exceeds expected range [0, {config['num_gloss']-1}]\")\n        if max_cat >= config['num_cat']:\n            raise ValueError(f\"Category label {max_cat} exceeds expected range [0, {config['num_cat']-1}]\")\n        \n        print(f\"Label validation passed:\")\n        print(f\"  - Gloss range: [0, {max_gloss}] (expected max: {config['num_gloss']-1})\")\n        print(f\"  - Category range: [0, {max_cat}] (expected max: {config['num_cat']-1})\")\n        \n        # Create dataset based on model type\n        if config['model_type'] == 'transformer':\n            # Use keypoint dataset for transformer\n            dataset = FSLKeypointFileDataset(\n                keypoints_dir=test_data_path,\n                labels_csv=test_labels_path,\n                kp_key=config['kp_key']\n            )\n            collate_fn = collate_keypoints_with_padding\n            print(\"Using keypoint dataset for Transformer model\")\n        elif config['model_type'] == 'iv3_gru':\n            # Use feature dataset for IV3-GRU\n            dataset = FSLFeatureFileDataset(\n                features_dir=test_data_path,\n                labels_csv=test_labels_path,\n                feature_key=config['feature_key']\n            )\n            collate_fn = collate_features_with_padding\n            print(\"Using feature dataset for IV3-GRU model\")\n        else:\n            raise ValueError(f\"Unknown model type: {config['model_type']}\")\n        \n        # Validate dataset has data\n        if len(dataset) == 0:\n            raise ValueError(\"Dataset is empty - check data paths and file formats\")\n        \n        # Test loading one sample to validate format\n        try:\n            sample = dataset[0]\n            print(f\"Dataset sample validation:\")\n            if len(sample) == 4:  # With lengths\n                X, gloss, cat, length = sample\n                print(f\"  - Input shape: {X.shape}\")\n                print(f\"  - Length: {length}\")\n            else:  # Without lengths\n                X, gloss, cat = sample\n                print(f\"  - Input shape: {X.shape}\")\n                print(f\"  - No length info\")\n            print(f\"  - Gloss: {gloss}, Category: {cat}\")\n        except Exception as e:\n            print(f\"WARNING: Failed to validate sample format: {e}\")\n        \n        # Create dataloader\n        dataloader = DataLoader(\n            dataset,\n            batch_size=config['batch_size'],\n            shuffle=False,  # Don't shuffle for consistent evaluation\n            num_workers=config['num_workers'],\n            collate_fn=collate_fn,\n            pin_memory=True if config['device'] == 'cuda' else False,\n            drop_last=False  # Keep all samples for evaluation\n        )\n        \n        print(f\"Test dataloader created with {len(dataloader)} batches\")\n        \n        # Display dataset statistics\n        print(f\"\\nTest dataset statistics:\")\n        print(f\"  - Total samples: {len(dataset)}\")\n        print(f\"  - Batch size: {config['batch_size']}\")\n        print(f\"  - Number of batches: {len(dataloader)}\")\n        print(f\"  - Gloss classes: {labels_df['gloss'].nunique()}\")\n        print(f\"  - Category classes: {labels_df['cat'].nunique()}\")\n        \n        # Show class distribution\n        print(f\"\\nClass distribution:\")\n        gloss_counts = labels_df['gloss'].value_counts().sort_index()\n        print(f\"  - Gloss distribution:\")\n        print(f\"    Range: {gloss_counts.min()} to {gloss_counts.max()} samples per class\")\n        print(f\"    Mean: {gloss_counts.mean():.1f} samples per class\")\n        print(f\"    Std: {gloss_counts.std():.1f} samples per class\")\n        \n        cat_counts = labels_df['cat'].value_counts().sort_index()\n        print(f\"  - Category distribution:\")\n        print(f\"    Range: {cat_counts.min()} to {cat_counts.max()} samples per class\")\n        print(f\"    Mean: {cat_counts.mean():.1f} samples per class\")\n        print(f\"    Std: {cat_counts.std():.1f} samples per class\")\n        \n        # Check for class imbalance\n        gloss_imbalance = gloss_counts.std() / gloss_counts.mean() if gloss_counts.mean() > 0 else 0\n        cat_imbalance = cat_counts.std() / cat_counts.mean() if cat_counts.mean() > 0 else 0\n        \n        if gloss_imbalance > 0.5:\n            print(f\"  WARNING: High gloss class imbalance (CV: {gloss_imbalance:.2f})\")\n        if cat_imbalance > 0.5:\n            print(f\"  WARNING: High category class imbalance (CV: {cat_imbalance:.2f})\")\n        \n        # Check for missing classes\n        expected_gloss_classes = set(range(config['num_gloss']))\n        actual_gloss_classes = set(labels_df['gloss'].unique())\n        missing_gloss = expected_gloss_classes - actual_gloss_classes\n        if missing_gloss:\n            print(f\"  WARNING: Missing gloss classes in test data: {sorted(missing_gloss)}\")\n        \n        expected_cat_classes = set(range(config['num_cat']))\n        actual_cat_classes = set(labels_df['cat'].unique())\n        missing_cat = expected_cat_classes - actual_cat_classes\n        if missing_cat:\n            print(f\"  WARNING: Missing category classes in test data: {sorted(missing_cat)}\")\n        \n        return dataset, dataloader, labels_df\n        \n    except FileNotFoundError as e:\n        print(f\"ERROR: {e}\")\n        print(\"Please check the test data paths in the configuration\")\n        raise\n    except ValueError as e:\n        print(f\"ERROR: Data validation failed: {e}\")\n        raise\n    except Exception as e:\n        print(f\"ERROR: Unexpected error loading test data: {type(e).__name__}: {e}\")\n        raise\n\n# Load test data with improved validation\nif config_valid:\n    try:\n        test_dataset, test_dataloader, test_labels_df = load_test_data(CONFIG)\n        print(\"Test data loading successful - ready for evaluation\")\n    except Exception as e:\n        print(f\"Failed to load test data: {e}\")\n        print(\"Please check your configuration and data files\")\n        test_dataset = None\n        test_dataloader = None\n        test_labels_df = None\nelse:\n    print(\"Skipping test data loading due to configuration validation failure.\")\n    test_dataset = None\n    test_dataloader = None\n    test_labels_df = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Run comprehensive evaluation on the test dataset and calculate detailed metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def calculate_confidence_intervals(accuracies, confidence_level=0.95):\n    \"\"\"Calculate confidence intervals for accuracy metrics using bootstrap\"\"\"\n    if len(accuracies) == 0:\n        return 0.0, 0.0\n    \n    n = len(accuracies)\n    alpha = 1 - confidence_level\n    lower_percentile = (alpha / 2) * 100\n    upper_percentile = (1 - alpha / 2) * 100\n    \n    lower_bound = np.percentile(accuracies, lower_percentile)\n    upper_bound = np.percentile(accuracies, upper_percentile)\n    \n    return lower_bound, upper_bound\n\ndef evaluate_model_comprehensive(model, dataloader, device, model_type, config):\n    \"\"\"Comprehensive model evaluation with detailed metrics and statistical analysis\"\"\"\n    print(f\"Running comprehensive evaluation...\")\n    \n    if model is None:\n        raise ValueError(\"Model is None - cannot evaluate\")\n    if dataloader is None:\n        raise ValueError(\"Dataloader is None - cannot evaluate\")\n    \n    model.eval()\n    device = torch.device(device)\n    \n    # Storage for predictions and targets\n    all_gloss_preds = []\n    all_cat_preds = []\n    all_gloss_targets = []\n    all_cat_targets = []\n    all_gloss_probs = []\n    all_cat_probs = []\n    \n    # Loss calculation\n    criterion = nn.CrossEntropyLoss()\n    total_loss = 0.0\n    num_batches = 0\n    \n    # Create forward function based on model type with better error handling\n    if model_type == 'transformer':\n        def forward_fn(m, X, lengths=None):\n            try:\n                if lengths is not None:\n                    B, T, _ = X.shape\n                    device = X.device\n                    time_indices = torch.arange(T, device=device).unsqueeze(0)\n                    mask = (time_indices < lengths.unsqueeze(1))\n                else:\n                    mask = None\n                return m(X, mask=mask)\n            except Exception as e:\n                print(f\"Error in transformer forward pass: {e}\")\n                raise\n    else:  # iv3_gru\n        def forward_fn(m, X, lengths=None):\n            try:\n                return m(X, lengths=lengths, features_already=True)\n            except Exception as e:\n                print(f\"Error in IV3-GRU forward pass: {e}\")\n                raise\n    \n    # Evaluation loop with progress tracking\n    total_samples = 0\n    with torch.no_grad():\n        for batch_idx, batch in enumerate(dataloader):\n            try:\n                if len(batch) == 4:\n                    X, gloss_targets, cat_targets, lengths = batch\n                    lengths = lengths.to(device)\n                else:\n                    X, gloss_targets, cat_targets = batch\n                    lengths = None\n                \n                X = X.to(device)\n                gloss_targets = gloss_targets.to(device)\n                cat_targets = cat_targets.to(device)\n                \n                # Validate input shapes\n                if X.dim() != 3:\n                    raise ValueError(f\"Expected 3D input tensor [B, T, D], got {X.shape}\")\n                \n                # Forward pass\n                gloss_logits, cat_logits = forward_fn(model, X, lengths)\n                \n                # Validate output shapes\n                if gloss_logits.dim() != 2 or cat_logits.dim() != 2:\n                    raise ValueError(f\"Invalid output dimensions: gloss {gloss_logits.shape}, cat {cat_logits.shape}\")\n                \n                # Calculate loss\n                loss_gloss = criterion(gloss_logits, gloss_targets)\n                loss_cat = criterion(cat_logits, cat_targets)\n                batch_loss = loss_gloss + loss_cat\n                total_loss += batch_loss.item()\n                num_batches += 1\n                total_samples += X.shape[0]\n                \n                # Get predictions\n                gloss_preds = gloss_logits.argmax(dim=1)\n                cat_preds = cat_logits.argmax(dim=1)\n                \n                # Get probabilities\n                gloss_probs = torch.softmax(gloss_logits, dim=1)\n                cat_probs = torch.softmax(cat_logits, dim=1)\n                \n                # Store results\n                all_gloss_preds.extend(gloss_preds.cpu().numpy())\n                all_cat_preds.extend(cat_preds.cpu().numpy())\n                all_gloss_targets.extend(gloss_targets.cpu().numpy())\n                all_cat_targets.extend(cat_targets.cpu().numpy())\n                all_gloss_probs.extend(gloss_probs.cpu().numpy())\n                all_cat_probs.extend(cat_probs.cpu().numpy())\n                \n                if (batch_idx + 1) % 10 == 0:\n                    print(f\"  Processed {batch_idx + 1}/{len(dataloader)} batches ({total_samples} samples)\")\n                    \n            except Exception as e:\n                print(f\"Error processing batch {batch_idx}: {e}\")\n                print(f\"Batch info: X shape: {X.shape if 'X' in locals() else 'Unknown'}\")\n                raise\n    \n    print(f\"Evaluation completed! Processed {total_samples} samples in {num_batches} batches\")\n    \n    # Convert to numpy arrays\n    all_gloss_preds = np.array(all_gloss_preds)\n    all_cat_preds = np.array(all_cat_preds)\n    all_gloss_targets = np.array(all_gloss_targets)\n    all_cat_targets = np.array(all_cat_targets)\n    all_gloss_probs = np.array(all_gloss_probs)\n    all_cat_probs = np.array(all_cat_probs)\n    \n    # Validate arrays\n    if len(all_gloss_preds) == 0:\n        raise ValueError(\"No predictions collected - check data loading and model forward pass\")\n    \n    # Calculate basic metrics\n    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n    gloss_accuracy = accuracy_score(all_gloss_targets, all_gloss_preds)\n    cat_accuracy = accuracy_score(all_cat_targets, all_cat_preds)\n    \n    # Top-k accuracy (with error handling)\n    try:\n        gloss_top3 = top_k_accuracy_score(all_gloss_targets, all_gloss_probs, k=min(3, all_gloss_probs.shape[1]))\n        gloss_top5 = top_k_accuracy_score(all_gloss_targets, all_gloss_probs, k=min(5, all_gloss_probs.shape[1]))\n    except Exception as e:\n        print(f\"Warning: Could not calculate top-k accuracy: {e}\")\n        gloss_top3 = gloss_accuracy\n        gloss_top5 = gloss_accuracy\n    \n    # Additional metrics\n    try:\n        gloss_kappa = cohen_kappa_score(all_gloss_targets, all_gloss_preds)\n        cat_kappa = cohen_kappa_score(all_cat_targets, all_cat_preds)\n        gloss_mcc = matthews_corrcoef(all_gloss_targets, all_gloss_preds)\n        cat_mcc = matthews_corrcoef(all_cat_targets, all_cat_preds)\n    except Exception as e:\n        print(f\"Warning: Could not calculate advanced metrics: {e}\")\n        gloss_kappa = cat_kappa = gloss_mcc = cat_mcc = 0.0\n    \n    # Bootstrap confidence intervals\n    n_samples = config.get('bootstrap_samples', 1000)\n    confidence_level = config.get('confidence_level', 0.95)\n    \n    print(f\"Calculating confidence intervals with {n_samples} bootstrap samples...\")\n    \n    # Bootstrap gloss accuracy\n    gloss_accuracies = []\n    for _ in range(n_samples):\n        indices = np.random.choice(len(all_gloss_targets), len(all_gloss_targets), replace=True)\n        sample_acc = accuracy_score(all_gloss_targets[indices], all_gloss_preds[indices])\n        gloss_accuracies.append(sample_acc)\n    \n    # Bootstrap category accuracy\n    cat_accuracies = []\n    for _ in range(n_samples):\n        indices = np.random.choice(len(all_cat_targets), len(all_cat_targets), replace=True)\n        sample_acc = accuracy_score(all_cat_targets[indices], all_cat_preds[indices])\n        cat_accuracies.append(sample_acc)\n    \n    # Calculate confidence intervals\n    gloss_ci_lower, gloss_ci_upper = calculate_confidence_intervals(gloss_accuracies, confidence_level)\n    cat_ci_lower, cat_ci_upper = calculate_confidence_intervals(cat_accuracies, confidence_level)\n    \n    print(f\"Evaluation completed!\")\n    print(f\"  - Average loss: {avg_loss:.4f}\")\n    print(f\"  - Gloss accuracy: {gloss_accuracy:.4f} [{gloss_ci_lower:.4f}, {gloss_ci_upper:.4f}]\")\n    print(f\"  - Category accuracy: {cat_accuracy:.4f} [{cat_ci_lower:.4f}, {cat_ci_upper:.4f}]\")\n    print(f\"  - Gloss top-3 accuracy: {gloss_top3:.4f}\")\n    print(f\"  - Gloss top-5 accuracy: {gloss_top5:.4f}\")\n    print(f\"  - Gloss Cohen's Kappa: {gloss_kappa:.4f}\")\n    print(f\"  - Category Cohen's Kappa: {cat_kappa:.4f}\")\n    print(f\"  - Gloss Matthews Correlation: {gloss_mcc:.4f}\")\n    print(f\"  - Category Matthews Correlation: {cat_mcc:.4f}\")\n    \n    return {\n        'loss': avg_loss,\n        'gloss_accuracy': gloss_accuracy,\n        'cat_accuracy': cat_accuracy,\n        'gloss_top3_accuracy': gloss_top3,\n        'gloss_top5_accuracy': gloss_top5,\n        'gloss_kappa': gloss_kappa,\n        'cat_kappa': cat_kappa,\n        'gloss_mcc': gloss_mcc,\n        'cat_mcc': cat_mcc,\n        'gloss_ci_lower': gloss_ci_lower,\n        'gloss_ci_upper': gloss_ci_upper,\n        'cat_ci_lower': cat_ci_lower,\n        'cat_ci_upper': cat_ci_upper,\n        'gloss_predictions': all_gloss_preds,\n        'cat_predictions': all_cat_preds,\n        'gloss_targets': all_gloss_targets,\n        'cat_targets': all_cat_targets,\n        'gloss_probabilities': all_gloss_probs,\n        'cat_probabilities': all_cat_probs\n    }\n\n# Run evaluation with better error handling\nif (config_valid and 'model' in locals() and model is not None and \n    'test_dataloader' in locals() and test_dataloader is not None):\n    try:\n        results = evaluate_model_comprehensive(\n            model, test_dataloader, CONFIG['device'], CONFIG['model_type'], CONFIG\n        )\n        print(\"Model evaluation successful!\")\n    except Exception as e:\n        print(f\"Failed to evaluate model: {e}\")\n        print(\"Please check your model, data, and configuration\")\n        results = None\nelse:\n    print(\"Skipping evaluation due to missing model or test data.\")\n    results = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Performance Summary\n",
    "\n",
    "Display key performance metrics in a clear summary format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_performance_summary(results, model_type, checkpoint_info=None):\n",
    "    \"\"\"Create a comprehensive performance summary with statistical analysis\"\"\"\n",
    "    \n",
    "    # Basic metrics with confidence intervals\n",
    "    summary_data = {\n",
    "        'Metric': [\n",
    "            'Test Loss',\n",
    "            'Gloss Accuracy',\n",
    "            'Category Accuracy', \n",
    "            'Gloss Top-3 Accuracy',\n",
    "            'Gloss Top-5 Accuracy',\n",
    "            'Gloss Cohen\\'s Kappa',\n",
    "            'Category Cohen\\'s Kappa',\n",
    "            'Gloss Matthews Correlation',\n",
    "            'Category Matthews Correlation'\n",
    "        ],\n",
    "        'Value': [\n",
    "            f\"{results['loss']:.4f}\",\n",
    "            f\"{results['gloss_accuracy']:.4f}\",\n",
    "            f\"{results['cat_accuracy']:.4f}\",\n",
    "            f\"{results['gloss_top3_accuracy']:.4f}\",\n",
    "            f\"{results['gloss_top5_accuracy']:.4f}\",\n",
    "            f\"{results['gloss_kappa']:.4f}\",\n",
    "            f\"{results['cat_kappa']:.4f}\",\n",
    "            f\"{results['gloss_mcc']:.4f}\",\n",
    "            f\"{results['cat_mcc']:.4f}\"\n",
    "        ],\n",
    "        'Percentage': [\n",
    "            f\"{results['loss']:.2f}\",\n",
    "            f\"{results['gloss_accuracy']*100:.2f}%\",\n",
    "            f\"{results['cat_accuracy']*100:.2f}%\",\n",
    "            f\"{results['gloss_top3_accuracy']*100:.2f}%\",\n",
    "            f\"{results['gloss_top5_accuracy']*100:.2f}%\",\n",
    "            f\"{results['gloss_kappa']*100:.2f}%\",\n",
    "            f\"{results['cat_kappa']*100:.2f}%\",\n",
    "            f\"{results['gloss_mcc']*100:.2f}%\",\n",
    "            f\"{results['cat_mcc']*100:.2f}%\"\n",
    "        ],\n",
    "        'Confidence Interval': [\n",
    "            'N/A',\n",
    "            f\"[{results['gloss_ci_lower']:.4f}, {results['gloss_ci_upper']:.4f}]\",\n",
    "            f\"[{results['cat_ci_lower']:.4f}, {results['cat_ci_upper']:.4f}]\",\n",
    "            'N/A',\n",
    "            'N/A',\n",
    "            'N/A',\n",
    "            'N/A',\n",
    "            'N/A',\n",
    "            'N/A'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    print(\"PERFORMANCE SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Model Type: {model_type.upper()}\")\n",
    "    if checkpoint_info:\n",
    "        print(f\"Training Epoch: {checkpoint_info.get('epoch', 'Unknown')}\")\n",
    "        print(f\"Best Validation Metric: {checkpoint_info.get('best_metric', 'Unknown'):.4f}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Display table\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # Additional insights\n",
    "    print(f\"\\nKey Insights:\")\n",
    "    print(f\"  • Gloss classification: {results['gloss_accuracy']*100:.1f}% accuracy\")\n",
    "    print(f\"    - 95% CI: [{results['gloss_ci_lower']*100:.1f}%, {results['gloss_ci_upper']*100:.1f}%]\")\n",
    "    print(f\"    - Cohen's Kappa: {results['gloss_kappa']:.3f} (agreement quality)\")\n",
    "    print(f\"    - Matthews Correlation: {results['gloss_mcc']:.3f} (balanced accuracy)\")\n",
    "    \n",
    "    print(f\"  • Category classification: {results['cat_accuracy']*100:.1f}% accuracy\")\n",
    "    print(f\"    - 95% CI: [{results['cat_ci_lower']*100:.1f}%, {results['cat_ci_upper']*100:.1f}%]\")\n",
    "    print(f\"    - Cohen's Kappa: {results['cat_kappa']:.3f} (agreement quality)\")\n",
    "    print(f\"    - Matthews Correlation: {results['cat_mcc']:.3f} (balanced accuracy)\")\n",
    "    \n",
    "    print(f\"  • Top-k performance:\")\n",
    "    print(f\"    - Top-3: {results['gloss_top3_accuracy']*100:.1f}% (good for ranking)\")\n",
    "    print(f\"    - Top-5: {results['gloss_top5_accuracy']*100:.1f}% (excellent for ranking)\")\n",
    "    \n",
    "    # Performance assessment\n",
    "    print(f\"\\nPerformance Assessment:\")\n",
    "    \n",
    "    # Gloss performance\n",
    "    if results['gloss_accuracy'] > 0.8:\n",
    "        print(f\"  • EXCELLENT gloss classification performance!\")\n",
    "    elif results['gloss_accuracy'] > 0.6:\n",
    "        print(f\"  • GOOD gloss classification performance\")\n",
    "    elif results['gloss_accuracy'] > 0.4:\n",
    "        print(f\"  • FAIR gloss classification performance\")\n",
    "    else:\n",
    "        print(f\"  • POOR gloss classification performance - needs improvement\")\n",
    "    \n",
    "    # Category performance\n",
    "    if results['cat_accuracy'] > 0.7:\n",
    "        print(f\"  • EXCELLENT category classification performance!\")\n",
    "    elif results['cat_accuracy'] > 0.5:\n",
    "        print(f\"  • GOOD category classification performance\")\n",
    "    elif results['cat_accuracy'] > 0.3:\n",
    "        print(f\"  • FAIR category classification performance\")\n",
    "    else:\n",
    "        print(f\"  • POOR category classification performance - needs improvement\")\n",
    "    \n",
    "    # Agreement quality assessment\n",
    "    if results['gloss_kappa'] > 0.8:\n",
    "        print(f\"  • EXCELLENT gloss agreement quality (Kappa > 0.8)\")\n",
    "    elif results['gloss_kappa'] > 0.6:\n",
    "        print(f\"  • GOOD gloss agreement quality (Kappa > 0.6)\")\n",
    "    elif results['gloss_kappa'] > 0.4:\n",
    "        print(f\"  • MODERATE gloss agreement quality (Kappa > 0.4)\")\n",
    "    else:\n",
    "        print(f\"  • POOR gloss agreement quality (Kappa < 0.4)\")\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "# Create and display summary\n",
    "if 'results' in locals():\n",
    "    summary_df = create_performance_summary(results, CONFIG['model_type'], checkpoint)\n",
    "else:\n",
    "    print(\"No results available for summary. Please run evaluation first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix Analysis\n",
    "\n",
    "Visualize confusion matrices for both gloss and category classification to understand misclassification patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(results, show_plots=True, save_plots=False):\n",
    "    \"\"\"Plot confusion matrices for gloss and category classification with enhanced analysis\"\"\"\n",
    "    \n",
    "    # Calculate confusion matrices\n",
    "    gloss_cm = confusion_matrix(results['gloss_targets'], results['gloss_predictions'])\n",
    "    cat_cm = confusion_matrix(results['cat_targets'], results['cat_predictions'])\n",
    "    \n",
    "    # Create subplots with better layout\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    \n",
    "    # Gloss confusion matrix (raw counts)\n",
    "    sns.heatmap(gloss_cm, annot=False, fmt='d', cmap='Blues', ax=ax1)\n",
    "    ax1.set_title('Gloss Classification Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Predicted Gloss Class')\n",
    "    ax1.set_ylabel('True Gloss Class')\n",
    "    \n",
    "    # Gloss confusion matrix (normalized)\n",
    "    gloss_cm_norm = gloss_cm.astype('float') / gloss_cm.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(gloss_cm_norm, annot=False, fmt='.3f', cmap='Blues', ax=ax2)\n",
    "    ax2.set_title('Gloss Classification Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Predicted Gloss Class')\n",
    "    ax2.set_ylabel('True Gloss Class')\n",
    "    \n",
    "    # Category confusion matrix (raw counts)\n",
    "    sns.heatmap(cat_cm, annot=True, fmt='d', cmap='Greens', ax=ax3)\n",
    "    ax3.set_title('Category Classification Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Predicted Category Class')\n",
    "    ax3.set_ylabel('True Category Class')\n",
    "    \n",
    "    # Category confusion matrix (normalized)\n",
    "    cat_cm_norm = cat_cm.astype('float') / cat_cm.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(cat_cm_norm, annot=True, fmt='.3f', cmap='Greens', ax=ax4)\n",
    "    ax4.set_title('Category Classification Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xlabel('Predicted Category Class')\n",
    "    ax4.set_ylabel('True Category Class')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    \n",
    "    if save_plots:\n",
    "        plt.savefig('confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"Confusion matrices saved as 'confusion_matrices.png'\")\n",
    "    \n",
    "    # Print confusion matrix statistics\n",
    "    print(\"Confusion Matrix Statistics:\")\n",
    "    print(f\"  Gloss matrix shape: {gloss_cm.shape}\")\n",
    "    print(f\"  Category matrix shape: {cat_cm.shape}\")\n",
    "    print(f\"  Gloss diagonal accuracy: {np.trace(gloss_cm) / np.sum(gloss_cm):.4f}\")\n",
    "    print(f\"  Category diagonal accuracy: {np.trace(cat_cm) / np.sum(cat_cm):.4f}\")\n",
    "    \n",
    "    # Additional analysis\n",
    "    print(f\"\\nDetailed Analysis:\")\n",
    "    \n",
    "    # Gloss analysis\n",
    "    gloss_diag = np.diag(gloss_cm)\n",
    "    gloss_row_sums = np.sum(gloss_cm, axis=1)\n",
    "    gloss_col_sums = np.sum(gloss_cm, axis=0)\n",
    "    \n",
    "    print(f\"  Gloss Classification:\")\n",
    "    print(f\"    - Best predicted class: {np.argmax(gloss_col_sums)} ({np.max(gloss_col_sums)} predictions)\")\n",
    "    print(f\"    - Most frequent true class: {np.argmax(gloss_row_sums)} ({np.max(gloss_row_sums)} samples)\")\n",
    "    print(f\"    - Most accurate class: {np.argmax(gloss_diag / gloss_row_sums)} ({np.max(gloss_diag / gloss_row_sums):.3f} accuracy)\")\n",
    "    print(f\"    - Least accurate class: {np.argmin(gloss_diag / gloss_row_sums)} ({np.min(gloss_diag / gloss_row_sums):.3f} accuracy)\")\n",
    "    \n",
    "    # Category analysis\n",
    "    cat_diag = np.diag(cat_cm)\n",
    "    cat_row_sums = np.sum(cat_cm, axis=1)\n",
    "    cat_col_sums = np.sum(cat_cm, axis=0)\n",
    "    \n",
    "    print(f\"  Category Classification:\")\n",
    "    print(f\"    - Best predicted class: {np.argmax(cat_col_sums)} ({np.max(cat_col_sums)} predictions)\")\n",
    "    print(f\"    - Most frequent true class: {np.argmax(cat_row_sums)} ({np.max(cat_row_sums)} samples)\")\n",
    "    print(f\"    - Most accurate class: {np.argmax(cat_diag / cat_row_sums)} ({np.max(cat_diag / cat_row_sums):.3f} accuracy)\")\n",
    "    print(f\"    - Least accurate class: {np.argmin(cat_diag / cat_row_sums)} ({np.min(cat_diag / cat_row_sums):.3f} accuracy)\")\n",
    "    \n",
    "    return gloss_cm, cat_cm\n",
    "\n",
    "# Plot confusion matrices\n",
    "if 'results' in locals():\n",
    "    gloss_cm, cat_cm = plot_confusion_matrices(results, show_plots=CONFIG['show_plots'], save_plots=CONFIG['save_plots'])\n",
    "else:\n",
    "    print(\"No results available for confusion matrix analysis. Please run evaluation first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Class Performance Analysis\n",
    "\n",
    "Analyze performance for individual classes to identify which classes are performing well and which need improvement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_per_class_performance(results, num_gloss_classes, num_cat_classes):\n",
    "    \"\"\"Analyze per-class performance metrics with comprehensive statistics\"\"\"\n",
    "    \n",
    "    # Calculate per-class metrics for gloss\n",
    "    gloss_report = classification_report(\n",
    "        results['gloss_targets'], \n",
    "        results['gloss_predictions'], \n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Calculate per-class metrics for category\n",
    "    cat_report = classification_report(\n",
    "        results['cat_targets'], \n",
    "        results['cat_predictions'], \n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Extract per-class data for gloss\n",
    "    gloss_classes = []\n",
    "    gloss_precision = []\n",
    "    gloss_recall = []\n",
    "    gloss_f1 = []\n",
    "    gloss_support = []\n",
    "    \n",
    "    for i in range(num_gloss_classes):\n",
    "        if str(i) in gloss_report:\n",
    "            gloss_classes.append(f\"Gloss {i}\")\n",
    "            gloss_precision.append(gloss_report[str(i)]['precision'])\n",
    "            gloss_recall.append(gloss_report[str(i)]['recall'])\n",
    "            gloss_f1.append(gloss_report[str(i)]['f1-score'])\n",
    "            gloss_support.append(gloss_report[str(i)]['support'])\n",
    "        else:\n",
    "            gloss_classes.append(f\"Gloss {i}\")\n",
    "            gloss_precision.append(0.0)\n",
    "            gloss_recall.append(0.0)\n",
    "            gloss_f1.append(0.0)\n",
    "            gloss_support.append(0)\n",
    "    \n",
    "    # Extract per-class data for category\n",
    "    cat_classes = []\n",
    "    cat_precision = []\n",
    "    cat_recall = []\n",
    "    cat_f1 = []\n",
    "    cat_support = []\n",
    "    \n",
    "    for i in range(num_cat_classes):\n",
    "        if str(i) in cat_report:\n",
    "            cat_classes.append(f\"Category {i}\")\n",
    "            cat_precision.append(cat_report[str(i)]['precision'])\n",
    "            cat_recall.append(cat_report[str(i)]['recall'])\n",
    "            cat_f1.append(cat_report[str(i)]['f1-score'])\n",
    "            cat_support.append(cat_report[str(i)]['support'])\n",
    "        else:\n",
    "            cat_classes.append(f\"Category {i}\")\n",
    "            cat_precision.append(0.0)\n",
    "            cat_recall.append(0.0)\n",
    "            cat_f1.append(0.0)\n",
    "            cat_support.append(0)\n",
    "    \n",
    "    # Create DataFrames\n",
    "    gloss_df = pd.DataFrame({\n",
    "        'Class': gloss_classes,\n",
    "        'Precision': gloss_precision,\n",
    "        'Recall': gloss_recall,\n",
    "        'F1-Score': gloss_f1,\n",
    "        'Support': gloss_support\n",
    "    })\n",
    "    \n",
    "    cat_df = pd.DataFrame({\n",
    "        'Class': cat_classes,\n",
    "        'Precision': cat_precision,\n",
    "        'Recall': cat_recall,\n",
    "        'F1-Score': cat_f1,\n",
    "        'Support': cat_support\n",
    "    })\n",
    "    \n",
    "    # Calculate additional statistics\n",
    "    gloss_df['Balanced_Accuracy'] = (gloss_df['Precision'] + gloss_df['Recall']) / 2\n",
    "    cat_df['Balanced_Accuracy'] = (cat_df['Precision'] + cat_df['Recall']) / 2\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(\"PER-CLASS PERFORMANCE SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(f\"\\nOverall Statistics:\")\n",
    "    print(f\"  Gloss Classification:\")\n",
    "    print(f\"    - Mean F1-Score: {gloss_df['F1-Score'].mean():.3f} ± {gloss_df['F1-Score'].std():.3f}\")\n",
    "    print(f\"    - Median F1-Score: {gloss_df['F1-Score'].median():.3f}\")\n",
    "    print(f\"    - Min F1-Score: {gloss_df['F1-Score'].min():.3f}\")\n",
    "    print(f\"    - Max F1-Score: {gloss_df['F1-Score'].max():.3f}\")\n",
    "    \n",
    "    print(f\"  Category Classification:\")\n",
    "    print(f\"    - Mean F1-Score: {cat_df['F1-Score'].mean():.3f} ± {cat_df['F1-Score'].std():.3f}\")\n",
    "    print(f\"    - Median F1-Score: {cat_df['F1-Score'].median():.3f}\")\n",
    "    print(f\"    - Min F1-Score: {cat_df['F1-Score'].min():.3f}\")\n",
    "    print(f\"    - Max F1-Score: {cat_df['F1-Score'].max():.3f}\")\n",
    "    \n",
    "    print(f\"\\nTop 10 Gloss Classes by F1-Score:\")\n",
    "    top_gloss = gloss_df.nlargest(10, 'F1-Score')\n",
    "    print(top_gloss[['Class', 'Precision', 'Recall', 'F1-Score', 'Support']].to_string(index=False, float_format='%.3f'))\n",
    "    \n",
    "    print(f\"\\nCategory Classification Performance:\")\n",
    "    print(cat_df[['Class', 'Precision', 'Recall', 'F1-Score', 'Support']].to_string(index=False, float_format='%.3f'))\n",
    "    \n",
    "    # Identify problematic classes\n",
    "    print(f\"\\nProblematic Gloss Classes (F1 < 0.5):\")\n",
    "    problematic_gloss = gloss_df[gloss_df['F1-Score'] < 0.5]\n",
    "    if len(problematic_gloss) > 0:\n",
    "        print(f\"  Found {len(problematic_gloss)} classes with poor performance:\")\n",
    "        print(problematic_gloss[['Class', 'Precision', 'Recall', 'F1-Score', 'Support']].to_string(index=False, float_format='%.3f'))\n",
    "    else:\n",
    "        print(\"  None found - all classes performing well!\")\n",
    "    \n",
    "    print(f\"\\nProblematic Category Classes (F1 < 0.5):\")\n",
    "    problematic_cat = cat_df[cat_df['F1-Score'] < 0.5]\n",
    "    if len(problematic_cat) > 0:\n",
    "        print(f\"  Found {len(problematic_cat)} classes with poor performance:\")\n",
    "        print(problematic_cat[['Class', 'Precision', 'Recall', 'F1-Score', 'Support']].to_string(index=False, float_format='%.3f'))\n",
    "    else:\n",
    "        print(\"  None found - all classes performing well!\")\n",
    "    \n",
    "    # Performance distribution analysis\n",
    "    print(f\"\\nPerformance Distribution Analysis:\")\n",
    "    \n",
    "    # Gloss performance categories\n",
    "    excellent_gloss = len(gloss_df[gloss_df['F1-Score'] >= 0.8])\n",
    "    good_gloss = len(gloss_df[(gloss_df['F1-Score'] >= 0.6) & (gloss_df['F1-Score'] < 0.8)])\n",
    "    fair_gloss = len(gloss_df[(gloss_df['F1-Score'] >= 0.4) & (gloss_df['F1-Score'] < 0.6)])\n",
    "    poor_gloss = len(gloss_df[gloss_df['F1-Score'] < 0.4])\n",
    "    \n",
    "    print(f\"  Gloss Classification Performance Distribution:\")\n",
    "    print(f\"    - Excellent (F1 ≥ 0.8): {excellent_gloss} classes ({excellent_gloss/len(gloss_df)*100:.1f}%)\")\n",
    "    print(f\"    - Good (0.6 ≤ F1 < 0.8): {good_gloss} classes ({good_gloss/len(gloss_df)*100:.1f}%)\")\n",
    "    print(f\"    - Fair (0.4 ≤ F1 < 0.6): {fair_gloss} classes ({fair_gloss/len(gloss_df)*100:.1f}%)\")\n",
    "    print(f\"    - Poor (F1 < 0.4): {poor_gloss} classes ({poor_gloss/len(gloss_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Category performance categories\n",
    "    excellent_cat = len(cat_df[cat_df['F1-Score'] >= 0.8])\n",
    "    good_cat = len(cat_df[(cat_df['F1-Score'] >= 0.6) & (cat_df['F1-Score'] < 0.8)])\n",
    "    fair_cat = len(cat_df[(cat_df['F1-Score'] >= 0.4) & (cat_df['F1-Score'] < 0.6)])\n",
    "    poor_cat = len(cat_df[cat_df['F1-Score'] < 0.4])\n",
    "    \n",
    "    print(f\"  Category Classification Performance Distribution:\")\n",
    "    print(f\"    - Excellent (F1 ≥ 0.8): {excellent_cat} classes ({excellent_cat/len(cat_df)*100:.1f}%)\")\n",
    "    print(f\"    - Good (0.6 ≤ F1 < 0.8): {good_cat} classes ({good_cat/len(cat_df)*100:.1f}%)\")\n",
    "    print(f\"    - Fair (0.4 ≤ F1 < 0.6): {fair_cat} classes ({fair_cat/len(cat_df)*100:.1f}%)\")\n",
    "    print(f\"    - Poor (F1 < 0.4): {poor_cat} classes ({poor_cat/len(cat_df)*100:.1f}%)\")\n",
    "    \n",
    "    return gloss_df, cat_df\n",
    "\n",
    "# Analyze per-class performance\n",
    "if 'results' in locals():\n",
    "    gloss_per_class, cat_per_class = analyze_per_class_performance(\n",
    "        results, \n",
    "        CONFIG['num_gloss'], \n",
    "        CONFIG['num_cat']\n",
    "    )\n",
    "else:\n",
    "    print(\"No results available for per-class analysis. Please run evaluation first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "Analyze misclassification patterns to understand what types of errors the model is making.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors(results, top_n=10):\n",
    "    \"\"\"Analyze misclassification patterns and errors with comprehensive statistics\"\"\"\n",
    "    \n",
    "    print(\"ERROR ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Gloss misclassifications\n",
    "    gloss_errors = results['gloss_targets'] != results['gloss_predictions']\n",
    "    gloss_error_rate = np.mean(gloss_errors)\n",
    "    \n",
    "    print(f\"Gloss Classification Errors:\")\n",
    "    print(f\"  - Error rate: {gloss_error_rate:.4f} ({gloss_error_rate*100:.2f}%)\")\n",
    "    print(f\"  - Total errors: {np.sum(gloss_errors)} out of {len(gloss_errors)}\")\n",
    "    print(f\"  - Correct predictions: {np.sum(~gloss_errors)} out of {len(gloss_errors)}\")\n",
    "    \n",
    "    # Category misclassifications\n",
    "    cat_errors = results['cat_targets'] != results['cat_predictions']\n",
    "    cat_error_rate = np.mean(cat_errors)\n",
    "    \n",
    "    print(f\"\\nCategory Classification Errors:\")\n",
    "    print(f\"  - Error rate: {cat_error_rate:.4f} ({cat_error_rate*100:.2f}%)\")\n",
    "    print(f\"  - Total errors: {np.sum(cat_errors)} out of {len(cat_errors)}\")\n",
    "    print(f\"  - Correct predictions: {np.sum(~cat_errors)} out of {len(cat_errors)}\")\n",
    "    \n",
    "    # Most confused class pairs for gloss\n",
    "    print(f\"\\nMost Confused Gloss Class Pairs:\")\n",
    "    gloss_confusion_pairs = {}\n",
    "    for true_class, pred_class in zip(results['gloss_targets'][gloss_errors], results['gloss_predictions'][gloss_errors]):\n",
    "        pair = (true_class, pred_class)\n",
    "        gloss_confusion_pairs[pair] = gloss_confusion_pairs.get(pair, 0) + 1\n",
    "    \n",
    "    # Sort by frequency\n",
    "    sorted_gloss_pairs = sorted(gloss_confusion_pairs.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"  Top {min(top_n, len(sorted_gloss_pairs))} most confused pairs:\")\n",
    "    for i, ((true_class, pred_class), count) in enumerate(sorted_gloss_pairs[:top_n]):\n",
    "        print(f\"    {i+1:2d}. True: {true_class:3d} → Pred: {pred_class:3d} ({count:3d} times)\")\n",
    "    \n",
    "    # Most confused class pairs for category\n",
    "    print(f\"\\nMost Confused Category Class Pairs:\")\n",
    "    cat_confusion_pairs = {}\n",
    "    for true_class, pred_class in zip(results['cat_targets'][cat_errors], results['cat_predictions'][cat_errors]):\n",
    "        pair = (true_class, pred_class)\n",
    "        cat_confusion_pairs[pair] = cat_confusion_pairs.get(pair, 0) + 1\n",
    "    \n",
    "    # Sort by frequency\n",
    "    sorted_cat_pairs = sorted(cat_confusion_pairs.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"  Top {min(top_n, len(sorted_cat_pairs))} most confused pairs:\")\n",
    "    for i, ((true_class, pred_class), count) in enumerate(sorted_cat_pairs[:top_n]):\n",
    "        print(f\"    {i+1:2d}. True: {true_class:3d} → Pred: {pred_class:3d} ({count:3d} times)\")\n",
    "    \n",
    "    # Confidence analysis\n",
    "    print(f\"\\nConfidence Analysis:\")\n",
    "    gloss_confidences = np.max(results['gloss_probabilities'], axis=1)\n",
    "    cat_confidences = np.max(results['cat_probabilities'], axis=1)\n",
    "    \n",
    "    print(f\"  Gloss confidence statistics:\")\n",
    "    print(f\"    - Mean: {np.mean(gloss_confidences):.4f}\")\n",
    "    print(f\"    - Median: {np.median(gloss_confidences):.4f}\")\n",
    "    print(f\"    - Std: {np.std(gloss_confidences):.4f}\")\n",
    "    print(f\"    - Min: {np.min(gloss_confidences):.4f}\")\n",
    "    print(f\"    - Max: {np.max(gloss_confidences):.4f}\")\n",
    "    print(f\"    - 25th percentile: {np.percentile(gloss_confidences, 25):.4f}\")\n",
    "    print(f\"    - 75th percentile: {np.percentile(gloss_confidences, 75):.4f}\")\n",
    "    \n",
    "    print(f\"  Category confidence statistics:\")\n",
    "    print(f\"    - Mean: {np.mean(cat_confidences):.4f}\")\n",
    "    print(f\"    - Median: {np.median(cat_confidences):.4f}\")\n",
    "    print(f\"    - Std: {np.std(cat_confidences):.4f}\")\n",
    "    print(f\"    - Min: {np.min(cat_confidences):.4f}\")\n",
    "    print(f\"    - Max: {np.max(cat_confidences):.4f}\")\n",
    "    print(f\"    - 25th percentile: {np.percentile(cat_confidences, 25):.4f}\")\n",
    "    print(f\"    - 75th percentile: {np.percentile(cat_confidences, 75):.4f}\")\n",
    "    \n",
    "    # Confidence vs accuracy analysis\n",
    "    print(f\"\\nConfidence vs Accuracy Analysis:\")\n",
    "    \n",
    "    # Define confidence thresholds\n",
    "    thresholds = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        high_conf_gloss = gloss_confidences > threshold\n",
    "        high_conf_cat = cat_confidences > threshold\n",
    "        \n",
    "        if np.sum(high_conf_gloss) > 0:\n",
    "            high_conf_gloss_acc = np.mean(results['gloss_targets'][high_conf_gloss] == results['gloss_predictions'][high_conf_gloss])\n",
    "            print(f\"  Gloss accuracy for confidence > {threshold}: {high_conf_gloss_acc:.4f} ({np.sum(high_conf_gloss)} samples)\")\n",
    "        \n",
    "        if np.sum(high_conf_cat) > 0:\n",
    "            high_conf_cat_acc = np.mean(results['cat_targets'][high_conf_cat] == results['cat_predictions'][high_conf_cat])\n",
    "            print(f\"  Category accuracy for confidence > {threshold}: {high_conf_cat_acc:.4f} ({np.sum(high_conf_cat)} samples)\")\n",
    "    \n",
    "    # Error pattern analysis\n",
    "    print(f\"\\nError Pattern Analysis:\")\n",
    "    \n",
    "    # Analyze errors by confidence level\n",
    "    low_conf_gloss = gloss_confidences < 0.5\n",
    "    high_conf_gloss = gloss_confidences > 0.8\n",
    "    \n",
    "    if np.sum(low_conf_gloss) > 0:\n",
    "        low_conf_error_rate = np.mean(gloss_errors[low_conf_gloss])\n",
    "        print(f\"  Low confidence gloss error rate (<0.5): {low_conf_error_rate:.4f}\")\n",
    "    \n",
    "    if np.sum(high_conf_gloss) > 0:\n",
    "        high_conf_error_rate = np.mean(gloss_errors[high_conf_gloss])\n",
    "        print(f\"  High confidence gloss error rate (>0.8): {high_conf_error_rate:.4f}\")\n",
    "    \n",
    "    # Analyze errors by class frequency\n",
    "    unique_gloss_targets, gloss_counts = np.unique(results['gloss_targets'], return_counts=True)\n",
    "    unique_cat_targets, cat_counts = np.unique(results['cat_targets'], return_counts=True)\n",
    "    \n",
    "    # Find rare classes (bottom 25% by frequency)\n",
    "    gloss_rare_threshold = np.percentile(gloss_counts, 25)\n",
    "    cat_rare_threshold = np.percentile(cat_counts, 25)\n",
    "    \n",
    "    rare_gloss_classes = unique_gloss_targets[gloss_counts <= gloss_rare_threshold]\n",
    "    rare_cat_classes = unique_cat_targets[cat_counts <= cat_rare_threshold]\n",
    "    \n",
    "    if len(rare_gloss_classes) > 0:\n",
    "        rare_gloss_mask = np.isin(results['gloss_targets'], rare_gloss_classes)\n",
    "        rare_gloss_error_rate = np.mean(gloss_errors[rare_gloss_mask])\n",
    "        print(f\"  Rare gloss classes error rate: {rare_gloss_error_rate:.4f}\")\n",
    "    \n",
    "    if len(rare_cat_classes) > 0:\n",
    "        rare_cat_mask = np.isin(results['cat_targets'], rare_cat_classes)\n",
    "        rare_cat_error_rate = np.mean(cat_errors[rare_cat_mask])\n",
    "        print(f\"  Rare category classes error rate: {rare_cat_error_rate:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'gloss_error_rate': gloss_error_rate,\n",
    "        'cat_error_rate': cat_error_rate,\n",
    "        'gloss_confusion_pairs': sorted_gloss_pairs,\n",
    "        'cat_confusion_pairs': sorted_cat_pairs,\n",
    "        'gloss_confidences': gloss_confidences,\n",
    "        'cat_confidences': cat_confidences,\n",
    "        'gloss_errors': gloss_errors,\n",
    "        'cat_errors': cat_errors\n",
    "    }\n",
    "\n",
    "# Analyze errors\n",
    "if 'results' in locals():\n",
    "    error_analysis = analyze_errors(results)\n",
    "else:\n",
    "    print(\"No results available for error analysis. Please run evaluation first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Visualizations\n",
    "\n",
    "Create interactive plots to explore the results in detail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_interactive_plots(results, error_analysis, gloss_per_class, cat_per_class, show_plots=True):\n    \"\"\"Create interactive visualizations using Plotly\"\"\"\n    \n    if not show_plots:\n        print(\"Interactive plots disabled in configuration\")\n        return\n    \n    # 1. Performance Metrics Comparison\n    fig1 = make_subplots(\n        rows=1, cols=2,\n        subplot_titles=('Gloss Classification', 'Category Classification'),\n        specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}]]\n    )\n    \n    # Gloss metrics\n    gloss_metrics = ['Accuracy', 'Top-3', 'Top-5']\n    gloss_values = [\n        results['gloss_accuracy'],\n        results['gloss_top3_accuracy'], \n        results['gloss_top5_accuracy']\n    ]\n    \n    fig1.add_trace(\n        go.Bar(x=gloss_metrics, y=gloss_values, name='Gloss', marker_color='lightblue'),\n        row=1, col=1\n    )\n    \n    # Category metrics\n    cat_metrics = ['Accuracy']\n    cat_values = [results['cat_accuracy']]\n    \n    fig1.add_trace(\n        go.Bar(x=cat_metrics, y=cat_values, name='Category', marker_color='lightgreen'),\n        row=1, col=2\n    )\n    \n    fig1.update_layout(\n        title=\"Model Performance Metrics\",\n        showlegend=True,\n        height=400\n    )\n    \n    fig1.show()\n    \n    # 2. Confidence Distribution\n    fig2 = make_subplots(\n        rows=1, cols=2,\n        subplot_titles=('Gloss Confidence Distribution', 'Category Confidence Distribution')\n    )\n    \n    # Gloss confidence histogram\n    fig2.add_trace(\n        go.Histogram(x=error_analysis['gloss_confidences'], nbinsx=30, name='Gloss Confidence'),\n        row=1, col=1\n    )\n    \n    # Category confidence histogram\n    fig2.add_trace(\n        go.Histogram(x=error_analysis['cat_confidences'], nbinsx=30, name='Category Confidence'),\n        row=1, col=2\n    )\n    \n    fig2.update_layout(\n        title=\"Confidence Score Distributions\",\n        showlegend=True,\n        height=400\n    )\n    \n    fig2.show()\n    \n    # 3. Per-Class F1 Scores (Top 20 for gloss) - only if data is available\n    if gloss_per_class is not None and not gloss_per_class.empty:\n        top_gloss_classes = gloss_per_class.nlargest(20, 'F1-Score')\n        \n        fig3 = go.Figure()\n        fig3.add_trace(go.Bar(\n            x=top_gloss_classes['Class'],\n            y=top_gloss_classes['F1-Score'],\n            name='F1-Score',\n            marker_color='lightcoral'\n        ))\n        \n        fig3.update_layout(\n            title=\"Top 20 Gloss Classes by F1-Score\",\n            xaxis_title=\"Class\",\n            yaxis_title=\"F1-Score\",\n            height=500\n        )\n        \n        fig3.show()\n    else:\n        print(\"Skipping per-class F1 plot - data not available\")\n    \n    # 4. Error Analysis - Most Confused Pairs\n    if len(error_analysis['gloss_confusion_pairs']) > 0:\n        top_confused = error_analysis['gloss_confusion_pairs'][:10]\n        true_classes = [f\"True: {pair[0][0]}\" for pair in top_confused]\n        pred_classes = [f\"Pred: {pair[0][1]}\" for pair in top_confused]\n        counts = [pair[1] for pair in top_confused]\n        \n        fig4 = go.Figure()\n        fig4.add_trace(go.Bar(\n            x=[f\"{t} → {p}\" for t, p in zip(true_classes, pred_classes)],\n            y=counts,\n            name='Confusion Count',\n            marker_color='orange'\n        ))\n        \n        fig4.update_layout(\n            title=\"Top 10 Most Confused Gloss Class Pairs\",\n            xaxis_title=\"True → Predicted\",\n            yaxis_title=\"Count\",\n            height=500,\n            xaxis_tickangle=-45\n        )\n        \n        fig4.show()\n    \n    print(\"Interactive plots created successfully!\")\n    print(\"Hover over the plots to see detailed information\")\n    print(\"Use the toolbar to zoom, pan, and download plots\")\n\n# Create interactive visualizations - now with proper parameter passing\nif 'results' in locals() and 'error_analysis' in locals():\n    # Check if per-class data is available\n    gloss_per_class_data = gloss_per_class if 'gloss_per_class' in locals() else None\n    cat_per_class_data = cat_per_class if 'cat_per_class' in locals() else None\n    \n    create_interactive_plots(\n        results, \n        error_analysis, \n        gloss_per_class_data, \n        cat_per_class_data, \n        show_plots=CONFIG['show_plots']\n    )\nelse:\n    print(\"Skipping interactive plots - missing required data (results or error_analysis)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison (Optional)\n",
    "\n",
    "Compare multiple models side-by-side if you have multiple checkpoints available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Model Comparison\n",
    "# Uncomment and modify this section if you want to compare multiple models\n",
    "\n",
    "def compare_models(model_configs, test_dataloader, device):\n",
    "    \"\"\"Compare multiple models side-by-side\"\"\"\n",
    "    \n",
    "    print(\"🔄 Comparing multiple models...\")\n",
    "    \n",
    "    comparison_results = []\n",
    "    \n",
    "    for config in model_configs:\n",
    "        print(f\"\\n📊 Evaluating {config['name']}...\")\n",
    "        \n",
    "        # Load model\n",
    "        model, checkpoint = load_model(\n",
    "            model_type=config['model_type'],\n",
    "            model_path=ROOT / config['model_path'],\n",
    "            device=device,\n",
    "            **{k: v for k, v in config.items() if k in ['num_gloss', 'num_cat', 'hidden1', 'hidden2', 'dropout']}\n",
    "        )\n",
    "        \n",
    "        # Evaluate model\n",
    "        results = evaluate_model_comprehensive(model, test_dataloader, device, config['model_type'])\n",
    "        \n",
    "        # Store results\n",
    "        comparison_results.append({\n",
    "            'name': config['name'],\n",
    "            'model_type': config['model_type'],\n",
    "            'gloss_accuracy': results['gloss_accuracy'],\n",
    "            'cat_accuracy': results['cat_accuracy'],\n",
    "            'gloss_top3_accuracy': results['gloss_top3_accuracy'],\n",
    "            'gloss_top5_accuracy': results['gloss_top5_accuracy'],\n",
    "            'loss': results['loss']\n",
    "        })\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_df = pd.DataFrame(comparison_results)\n",
    "    \n",
    "    print(\"\\n📊 MODEL COMPARISON RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(comparison_df.to_string(index=False, float_format='%.4f'))\n",
    "    \n",
    "    # Create comparison plot\n",
    "    if CONFIG['show_plots']:\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('Gloss Accuracy', 'Category Accuracy', 'Top-3 Accuracy', 'Loss'),\n",
    "            specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}], [{\"type\": \"bar\"}, {\"type\": \"bar\"}]]\n",
    "        )\n",
    "        \n",
    "        # Gloss accuracy\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=comparison_df['name'], y=comparison_df['gloss_accuracy'], name='Gloss Accuracy'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Category accuracy\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=comparison_df['name'], y=comparison_df['cat_accuracy'], name='Category Accuracy'),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Top-3 accuracy\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=comparison_df['name'], y=comparison_df['gloss_top3_accuracy'], name='Top-3 Accuracy'),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Loss\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=comparison_df['name'], y=comparison_df['loss'], name='Loss'),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"Model Comparison\",\n",
    "            showlegend=False,\n",
    "            height=600\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Example model comparison (uncomment to use)\n",
    "# model_configs = [\n",
    "#     {\n",
    "#         'name': 'Transformer Best',\n",
    "#         'model_type': 'transformer',\n",
    "#         'model_path': 'data/processed/SignTransformer_best.pt',\n",
    "#         'num_gloss': 105,\n",
    "#         'num_cat': 10\n",
    "#     },\n",
    "#     {\n",
    "#         'name': 'IV3-GRU Best',\n",
    "#         'model_type': 'iv3_gru',\n",
    "#         'model_path': 'data/processed/InceptionV3GRU_best.pt',\n",
    "#         'num_gloss': 105,\n",
    "#         'num_cat': 10,\n",
    "#         'hidden1': 16,\n",
    "#         'hidden2': 12,\n",
    "#         'dropout': 0.3\n",
    "#     }\n",
    "# ]\n",
    "# \n",
    "# comparison_df = compare_models(model_configs, test_dataloader, CONFIG['device'])\n",
    "\n",
    "print(\"💡 To compare multiple models, uncomment and modify the model_configs section above\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results\n",
    "\n",
    "Export evaluation results to files for further analysis and reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results(results, summary_df, gloss_per_class, cat_per_class, error_analysis, config, output_dir='evaluation_results'):\n",
    "    \"\"\"Export all evaluation results to files\"\"\"\n",
    "    \n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    print(f\"Exporting results to {output_dir}/\")\n",
    "    \n",
    "    # Export summary metrics\n",
    "    summary_file = f\"{output_dir}/summary_metrics_{timestamp}.csv\"\n",
    "    summary_df.to_csv(summary_file, index=False)\n",
    "    print(f\"  - Summary metrics: {summary_file}\")\n",
    "    \n",
    "    # Export per-class performance\n",
    "    gloss_per_class_file = f\"{output_dir}/gloss_per_class_{timestamp}.csv\"\n",
    "    cat_per_class_file = f\"{output_dir}/cat_per_class_{timestamp}.csv\"\n",
    "    gloss_per_class.to_csv(gloss_per_class_file, index=False)\n",
    "    cat_per_class.to_csv(cat_per_class_file, index=False)\n",
    "    print(f\"  - Gloss per-class: {gloss_per_class_file}\")\n",
    "    print(f\"  - Category per-class: {cat_per_class_file}\")\n",
    "    \n",
    "    # Export detailed results\n",
    "    detailed_results = {\n",
    "        'config': config,\n",
    "        'timestamp': timestamp,\n",
    "        'model_type': config['model_type'],\n",
    "        'test_samples': len(results['gloss_targets']),\n",
    "        'gloss_accuracy': results['gloss_accuracy'],\n",
    "        'cat_accuracy': results['cat_accuracy'],\n",
    "        'gloss_top3_accuracy': results['gloss_top3_accuracy'],\n",
    "        'gloss_top5_accuracy': results['gloss_top5_accuracy'],\n",
    "        'gloss_kappa': results['gloss_kappa'],\n",
    "        'cat_kappa': results['cat_kappa'],\n",
    "        'gloss_mcc': results['gloss_mcc'],\n",
    "        'cat_mcc': results['cat_mcc'],\n",
    "        'gloss_ci_lower': results['gloss_ci_lower'],\n",
    "        'gloss_ci_upper': results['gloss_ci_upper'],\n",
    "        'cat_ci_lower': results['cat_ci_lower'],\n",
    "        'cat_ci_upper': results['cat_ci_upper'],\n",
    "        'gloss_error_rate': error_analysis['gloss_error_rate'],\n",
    "        'cat_error_rate': error_analysis['cat_error_rate'],\n",
    "        'gloss_confusion_pairs': error_analysis['gloss_confusion_pairs'][:20],  # Top 20\n",
    "        'cat_confusion_pairs': error_analysis['cat_confusion_pairs'][:20],  # Top 20\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    results_file = f\"{output_dir}/detailed_results_{timestamp}.json\"\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(detailed_results, f, indent=2, default=str)\n",
    "    print(f\"  - Detailed results: {results_file}\")\n",
    "    \n",
    "    # Export predictions\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'gloss_target': results['gloss_targets'],\n",
    "        'gloss_prediction': results['gloss_predictions'],\n",
    "        'cat_target': results['cat_targets'],\n",
    "        'cat_prediction': results['cat_predictions'],\n",
    "        'gloss_confidence': np.max(results['gloss_probabilities'], axis=1),\n",
    "        'cat_confidence': np.max(results['cat_probabilities'], axis=1),\n",
    "        'gloss_correct': results['gloss_targets'] == results['gloss_predictions'],\n",
    "        'cat_correct': results['cat_targets'] == results['cat_predictions']\n",
    "    })\n",
    "    \n",
    "    predictions_file = f\"{output_dir}/predictions_{timestamp}.csv\"\n",
    "    predictions_df.to_csv(predictions_file, index=False)\n",
    "    print(f\"  - Predictions: {predictions_file}\")\n",
    "    \n",
    "    # Create a comprehensive report\n",
    "    report_file = f\"{output_dir}/evaluation_report_{timestamp}.txt\"\n",
    "    with open(report_file, 'w') as f:\n",
    "        f.write(\"MODEL EVALUATION REPORT\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        f.write(f\"Model Type: {config['model_type'].upper()}\\n\")\n",
    "        f.write(f\"Evaluation Date: {timestamp}\\n\")\n",
    "        f.write(f\"Test Samples: {len(results['gloss_targets'])}\\n\\n\")\n",
    "        \n",
    "        f.write(\"PERFORMANCE METRICS\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        f.write(f\"Gloss Accuracy: {results['gloss_accuracy']:.4f} [{results['gloss_ci_lower']:.4f}, {results['gloss_ci_upper']:.4f}]\\n\")\n",
    "        f.write(f\"Category Accuracy: {results['cat_accuracy']:.4f} [{results['cat_ci_lower']:.4f}, {results['cat_ci_upper']:.4f}]\\n\")\n",
    "        f.write(f\"Gloss Top-3 Accuracy: {results['gloss_top3_accuracy']:.4f}\\n\")\n",
    "        f.write(f\"Gloss Top-5 Accuracy: {results['gloss_top5_accuracy']:.4f}\\n\")\n",
    "        f.write(f\"Gloss Cohen's Kappa: {results['gloss_kappa']:.4f}\\n\")\n",
    "        f.write(f\"Category Cohen's Kappa: {results['cat_kappa']:.4f}\\n\")\n",
    "        f.write(f\"Gloss Matthews Correlation: {results['gloss_mcc']:.4f}\\n\")\n",
    "        f.write(f\"Category Matthews Correlation: {results['cat_mcc']:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"ERROR ANALYSIS\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        f.write(f\"Gloss Error Rate: {error_analysis['gloss_error_rate']:.4f}\\n\")\n",
    "        f.write(f\"Category Error Rate: {error_analysis['cat_error_rate']:.4f}\\n\")\n",
    "        f.write(f\"Total Gloss Errors: {np.sum(error_analysis['gloss_errors'])}\\n\")\n",
    "        f.write(f\"Total Category Errors: {np.sum(error_analysis['cat_errors'])}\\n\\n\")\n",
    "        \n",
    "        f.write(\"TOP CONFUSED CLASS PAIRS\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        f.write(\"Gloss Classification:\\n\")\n",
    "        for i, ((true_class, pred_class), count) in enumerate(error_analysis['gloss_confusion_pairs'][:10]):\n",
    "            f.write(f\"  {i+1:2d}. True: {true_class:3d} → Pred: {pred_class:3d} ({count:3d} times)\\n\")\n",
    "        \n",
    "        f.write(\"\\nCategory Classification:\\n\")\n",
    "        for i, ((true_class, pred_class), count) in enumerate(error_analysis['cat_confusion_pairs'][:10]):\n",
    "            f.write(f\"  {i+1:2d}. True: {true_class:3d} → Pred: {pred_class:3d} ({count:3d} times)\\n\")\n",
    "    \n",
    "    print(f\"  - Evaluation report: {report_file}\")\n",
    "    \n",
    "    print(f\"\\nExport completed! All files saved to {output_dir}/\")\n",
    "    return output_dir\n",
    "\n",
    "# Export results\n",
    "if 'results' in locals() and 'summary_df' in locals():\n",
    "    export_dir = export_results(\n",
    "        results, \n",
    "        summary_df, \n",
    "        gloss_per_class, \n",
    "        cat_per_class, \n",
    "        error_analysis, \n",
    "        CONFIG\n",
    "    )\n",
    "else:\n",
    "    print(\"No results available for export. Please run evaluation first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "This notebook provides comprehensive evaluation of your trained FSLR models with statistical rigor and detailed analysis.\n",
    "\n",
    "### What we evaluated:\n",
    "- **Overall performance**: Accuracy, loss, top-k accuracy with confidence intervals\n",
    "- **Statistical analysis**: Cohen's Kappa, Matthews Correlation Coefficient, bootstrap confidence intervals\n",
    "- **Per-class analysis**: Precision, recall, F1-score with performance distribution analysis\n",
    "- **Confusion matrices**: Both raw counts and normalized versions with detailed analysis\n",
    "- **Comprehensive error analysis**: Misclassification patterns, confidence analysis, and error rate by class frequency\n",
    "- **Interactive visualizations**: Detailed plots for exploration and analysis\n",
    "- **Export functionality**: Complete results export for further analysis and reporting\n",
    "\n",
    "### Key insights you can gain:\n",
    "- Model performance on test data with statistical confidence\n",
    "- Which classes are performing well/poorly with detailed metrics\n",
    "- Common misclassification patterns and their frequencies\n",
    "- Confidence score distributions and their relationship to accuracy\n",
    "- Performance distribution across all classes\n",
    "- Error patterns by confidence level and class frequency\n",
    "\n",
    "### Next steps you might consider:\n",
    "1. **Improve problematic classes**: Focus on classes with low F1-scores or high error rates\n",
    "2. **Data augmentation**: Add more training data for confused class pairs\n",
    "3. **Model architecture**: Try different architectures or hyperparameters\n",
    "4. **Ensemble methods**: Combine multiple models for better performance\n",
    "5. **Error analysis**: Investigate specific misclassified examples using the exported predictions\n",
    "6. **Statistical validation**: Use the confidence intervals to assess model reliability\n",
    "\n",
    "### Tips for using this enhanced notebook:\n",
    "- Change the configuration cell to evaluate different models\n",
    "- Use the model comparison section to compare multiple models\n",
    "- Export results automatically for further analysis\n",
    "- Modify the analysis functions to focus on specific aspects\n",
    "- Use the exported CSV files for additional analysis in other tools\n",
    "- Check the evaluation report for a comprehensive summary\n",
    "\n",
    "### Files generated:\n",
    "- `summary_metrics_[timestamp].csv`: Main performance metrics\n",
    "- `gloss_per_class_[timestamp].csv`: Per-class gloss performance\n",
    "- `cat_per_class_[timestamp].csv`: Per-class category performance\n",
    "- `predictions_[timestamp].csv`: All predictions with confidence scores\n",
    "- `detailed_results_[timestamp].json`: Complete results in JSON format\n",
    "- `evaluation_report_[timestamp].txt`: Human-readable summary report\n",
    "\n",
    "**Happy evaluating! 🎉**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}